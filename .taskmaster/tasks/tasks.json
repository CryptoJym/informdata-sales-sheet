{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Create overall sales collateral task list for InformData pricing sheet",
        "description": "Define and seed a coherent Task Master work breakdown for producing the InformData pricing sales collateral, aligned to the existing repo structure and PRD. This establishes task groupings, dependencies, and acceptance criteria that the team will execute afterward.",
        "details": "Context and alignment\n- Source PRD: `prd_sales_sheet.txt` (objective/requirements already captured).\n- Task Master is configured: `.taskmaster/config.json` (enableCodebaseAnalysis=true) and workflow documented in `AGENTS.md` (core commands, generate, dependency validation).\n- No existing tasks found under `.taskmaster/tasks/`; this task creates the initial structured backlog only (no content authoring).\n\nPlan (Due‑Process compliant)\n- Preflight: `task-master list`, then `task-master validate-dependencies` (expect no tasks yet). Do not re‑init (per AGENTS/Playbook).\n- Parse PRD to draft tasks: `task-master parse-prd prd_sales_sheet.txt --num 12 --research` to seed initial candidates.\n- Curate into a clean WBS with explicit acceptance criteria and dependencies, using only Task Master commands (no manual edits to `.taskmaster/tasks/tasks.json`).\n- Generate mirrors after edits: `task-master generate` to ensure markdown mirrors under `.taskmaster/tasks/` are created/updated.\n\nBacklog structure to create (top-level tasks with brief acceptance criteria)\n- Data sourcing (InformData costs): Define `data/pricing/informdata_costs.csv` schema (service_code, description, unit, cost_usd, source_doc, as_of_date). AC: file exists, schema validated, sources logged.\n- Price computation (+$1 AI/SaaS margin): Create a simple computation plan (spreadsheet or script path proposal) and document formula rules (cost + 1.00 USD per line item). AC: sample calculation table in `docs/sales/pricing_sheet.md` shows correct deltas.\n- Competitor MSRPs capture: Define `data/pricing/competitors.csv` (vendor, service_code, description, msrp_usd, source_url, as_of_date, notes). AC: at least 5 representative services populated with citations.\n- Sales collateral drafting: Create `docs/sales/pricing_sheet.md` skeleton (intro, pricing tables, value props, use cases, disclaimers). AC: section stubs committed, links to data files added.\n- Review and compliance checklist: Add checklist for finance/legal review within the task details. AC: checklist present and owners assigned.\n- Export-to-Gamma prep (optional): Outline export steps (manual or API) and identify any `GAMMA_API_KEY` addition to `.env.example` if API route is chosen. AC: export steps documented, not executed here.\n\nHow to apply with Task Master\n- Convert seeded items into explicit tasks with titles like: “Ingest InformData cost data”, “Compute in-house pricing (+$1)”, “Capture competitor MSRPs”, “Draft pricing sheet content”, “Review & signoff”, “Prepare Gamma export”.\n- For each, attach acceptance criteria and deliverable paths as above via: `task-master update-task --id=<id> --prompt=\"Add AC, deliverables, owners, and repo paths\"`.\n- Add minimal dependencies only where truly prerequisite:\n  - “Compute in-house pricing” depends on “Ingest InformData cost data”.\n  - “Draft pricing sheet content” depends on both “Compute in-house pricing” and “Capture competitor MSRPs”.\n  - “Review & signoff” depends on “Draft pricing sheet content”.\n  - “Prepare Gamma export” depends on “Review & signoff”.\n- After curating: `task-master validate-dependencies` then `task-master generate`.\n\nNotes and constraints\n- Never run `task-master init` (already configured). Use CLI only; do not hand-edit `.taskmaster/tasks/tasks.json`.\n- Keep tasks small and reversible; prefer plain CSV/Markdown in-repo deliverables (`docs/`, `data/`).\n- Reference files explicitly in each task’s details to anchor outputs to this repo.\n",
        "testStrategy": "Verification steps\n- Run `task-master list` and confirm new top-level tasks exist with clear, action-oriented titles as outlined.\n- Pick each task and `task-master show <id>`; verify acceptance criteria mention concrete repo paths: `data/pricing/informdata_costs.csv`, `data/pricing/competitors.csv`, `docs/sales/pricing_sheet.md`.\n- Validate dependency graph: `task-master validate-dependencies` reports no cycles; spot-check that drafting depends on both computation and MSRPs, and export depends on review.\n- Generate mirrors: `task-master generate`, then confirm `.taskmaster/tasks/` contains individual task files reflecting the curated details.\n- Sanity check: ensure no instructions tell contributors to manually edit `.taskmaster/tasks/tasks.json` or run `task-master init` again.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Assemble detailed task breakdown",
            "description": "Generate structured task list covering data sourcing, pricing computations, competitor research, collateral drafting, review, and Gamma export steps.",
            "details": "<info added on 2025-10-30T10:59:31.427Z>\nPlan of record for Subtask 1.1 (assemble detailed task breakdown)\n\n1) Repo and PRD review\n- Open prd_sales_sheet.txt and extract must-have sections (SKUs, pricing logic, competitor comparison, collateral structure, review gates, export requirements).\n- Confirm .taskmaster/config.json and AGENTS.md are present; enableCodebaseAnalysis=true.\n- Ensure directories exist or will be created: data/pricing/, scripts/, templates/, content/pricing/, docs/review/, exports/gamma/, docs/data_schemas/.\n\n2) Concrete workstreams to create\n\nWorkstream: Ingest and normalize pricing data\n- Deliverables: data/pricing/informdata_costs.csv; docs/data_schemas/pricing.yml; scripts/validate_pricing_data.py\n- Acceptance criteria: CSV schema defined and committed (sku, description, unit_cost, uom, source, updated_at). Validation script exits 0 on valid data and fails on schema mismatch. Sample seeded rows present. Paths referenced in acceptance.\n- Dependencies: PRD reviewed\n\nWorkstream: Margin computation and pricing tiers\n- Deliverables: scripts/compute_margins.py; data/pricing/informdata_prices.csv; tests/test_margin.py\n- Acceptance criteria: Deterministic margin calculations per PRD with unit tests covering tiering and rounding rules. Output CSV includes sku, unit_cost, margin_pct, tier, target_price. Re-runnable with same inputs produces identical outputs.\n- Dependencies: Ingest and normalize pricing data\n\nWorkstream: Competitor pricing dataset\n- Deliverables: data/pricing/competitors.csv; docs/research/competitor_sources.md\n- Acceptance criteria: Competitor CSV schema defined (competitor, sku, price, notes, collected_at). Sources and collection method documented. File referenced in acceptance.\n- Dependencies: PRD reviewed\n\nWorkstream: Collateral drafting (content + template)\n- Deliverables: content/pricing/informdata_pricing.md; templates/pricing_sheet.j2; assets/ (as needed)\n- Acceptance criteria: Draft covers PRD sections (value prop, SKU list, pricing tiers, competitor compare). Template binds to data/pricing/informdata_prices.csv and data/pricing/competitors.csv variables without errors.\n- Dependencies: Margin computation and pricing tiers; Competitor pricing dataset\n\nWorkstream: Review and signoff\n- Deliverables: docs/review/checklist_pricing_collateral.md; .taskmaster/approvals.json\n- Acceptance criteria: Checklist completed; approvals recorded from Product, Sales, and Finance with names, dates, and version hash of collateral file.\n- Dependencies: Collateral drafting\n\nWorkstream: Gamma export\n- Deliverables: scripts/export/gamma_export.py; exports/gamma/informdata_pricing_deck.json; exports/gamma/informdata_pricing.pdf\n- Acceptance criteria: One command generates both JSON for Gamma and a PDF export from the template and data. Links/IDs for Gamma doc captured in approvals.json.\n- Dependencies: Collateral drafting (final PDF gated by Review and signoff)\n\n3) Commands to create tasks (run in order, then wire dependencies)\n- task-master add-task --title \"Ingest and normalize pricing data\" --description \"Define CSV schema, seed data, and add validation for InformData costs.\" --deliverables \"data/pricing/informdata_costs.csv; docs/data_schemas/pricing.yml; scripts/validate_pricing_data.py\" --acceptance \"Schema defined and validated; sample rows committed; validation script passes/fails appropriately; acceptance references paths.\"\n- task-master add-task --title \"Margin computation and pricing tiers\" --description \"Implement margin model and tiered pricing outputs with tests.\" --deliverables \"scripts/compute_margins.py; data/pricing/informdata_prices.csv; tests/test_margin.py\" --acceptance \"Deterministic calculations per PRD; unit tests passing; output CSV contains required columns.\"\n- task-master add-task --title \"Competitor pricing dataset\" --description \"Collect and normalize competitor prices; document sources.\" --deliverables \"data/pricing/competitors.csv; docs/research/competitor_sources.md\" --acceptance \"CSV schema defined; sources documented; file paths referenced.\"\n- task-master add-task --title \"Collateral drafting (content + template)\" --description \"Author pricing collateral and template bound to data files.\" --deliverables \"content/pricing/informdata_pricing.md; templates/pricing_sheet.j2\" --acceptance \"Draft covers PRD sections; template renders with data/pricing/informdata_prices.csv and data/pricing/competitors.csv.\"\n- task-master add-task --title \"Review and signoff\" --description \"Run checklist and capture approvals from Product, Sales, Finance.\" --deliverables \"docs/review/checklist_pricing_collateral.md; .taskmaster/approvals.json\" --acceptance \"Checklist complete; approvals recorded with names/dates/version hash.\"\n- task-master add-task --title \"Gamma export\" --description \"Automate export to Gamma JSON and PDF.\" --deliverables \"scripts/export/gamma_export.py; exports/gamma/informdata_pricing_deck.json; exports/gamma/informdata_pricing.pdf\" --acceptance \"Single command produces both outputs; Gamma link/ID logged.\"\n\n4) Set dependencies and validate graph\n- After creation, capture IDs with task-master list, then:\n  - task-master set-deps <Margin_ID> <Ingest_ID>\n  - task-master set-deps <Collateral_ID> <Margin_ID> <Competitors_ID>\n  - task-master set-deps <Review_ID> <Collateral_ID>\n  - task-master set-deps <Gamma_ID> <Collateral_ID> <Review_ID>\n- Validate: task-master validate-graph\n\n5) Generate task mirrors for execution under Task 2\n- Create mirrors of Collateral drafting and Gamma export under Task 2:\n  - task-master mirror create --source <Collateral_ID> --parent 2\n  - task-master mirror create --source <Gamma_ID> --parent 2\n- Re-run task-master list and task-master validate-graph to confirm mirrors and dependencies resolve.\n</info added on 2025-10-30T10:59:31.427Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1,
            "updatedAt": "2025-10-30T11:29:58.238Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T11:29:58.238Z"
      },
      {
        "id": "2",
        "title": "Build InformData pricing sales collateral (content, data sources, and generation pipeline)",
        "description": "Author the InformData pricing sales collateral from the PRD, implement data sources and templates, and establish an automated build pipeline to generate Markdown and PDF outputs. Leverage the existing consolidated dataset and the drafted sales sheet to complete the templated, automated pipeline and align all artifacts to the agreed repo structure and acceptance criteria.",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "high",
        "details": "Scope\n- Produce a complete, publishable InformData pricing sales collateral package driven by structured data and templates, aligned to the PRD (prd_sales_sheet.txt) and the repo conventions defined in Task 1.\n- Integrate current progress: a consolidated pricing dataset and a manually authored sales sheet, migrating them into the canonical data and generation pipeline.\n\nCurrent progress (to integrate)\n- Consolidated dataset: content/pricing/informdata_pricing_table.csv (generated)\n- Draft sales sheet: docs/sales/pricing_sheet.md (authored)\n- Gamma deck (reference collateral): https://gamma.app/docs/zdlm8b8chaqwk0y\n\nInputs and references\n- PRD: prd_sales_sheet.txt (authoritative requirements and messaging).\n- Task 1 acceptance criteria and repo path conventions must be honored.\n- Task 3 and Task 11 schemas/validators should be used where applicable.\n\nRepository structure and artifacts to add or align\n- Data (CSV) [canonical targets used by the generator]:\n  - data/pricing/informdata_costs.csv (columns: sku, description, unit, base_price_usd, tier, min_qty, max_qty, notes)\n  - data/pricing/competitors.csv (columns: competitor, sku_equiv, base_price_usd, notes, source_url, last_verified_at)\n  - data/pricing/discounts.csv (columns: tier, min_qty, max_qty, pct_discount)\n  - data/pricing/fees.csv (columns: fee_type, description, price_usd, applies_to, notes)\n- Preexisting source (to normalize or support as interim input):\n  - content/pricing/informdata_pricing_table.csv (consolidated pricing table)\n- Narrative/config (YAML/MD):\n  - content/sales/informdata_pricing.yml (headline, subhead, highlights[], value_props[], faqs[], contact_cta, legal_ref)\n  - content/sales/legal_disclaimer.md (standard legal text referenced by the template)\n- Templates:\n  - templates/sales/pricing_sheet.md.j2 (Jinja2): sections for hero, value props, pricing tables, add-ons/fees, competitor comparison, FAQs, CTA, legal\n- Generation script and tooling:\n  - scripts/generate_pricing_sheet.py (Python 3.11; deps: pandas, jinja2, pyyaml, markdown-it-py or use Pandoc for PDF)\n  - requirements.txt (pin versions)\n  - Makefile targets: pricing (generate MD + PDF), clean (remove build artifacts)\n- Outputs:\n  - docs/sales/informdata_pricing.md (generated)\n  - dist/sales/informdata_pricing_{YYYYMMDD}.pdf (generated via Pandoc or md-to-pdf)\n- Assets:\n  - assets/images/sales/informdata_logo.svg\n  - assets/images/sales/icons/*.svg (optional, with alt text documented in YAML)\n- CI:\n  - .github/workflows/build-sales-collateral.yml to build on PRs and main, upload PDF as artifact, lint/validate data\n- Documentation:\n  - docs/sales/README.md (how to update data, run generation, and acceptance checklist). Include a link to the Gamma deck as an additional reference resource.\n\nImplementation steps\n1) Planning and validation\n- Read prd_sales_sheet.txt and extract required sections and messaging. Confirm Task 1’s defined acceptance criteria and file paths (e.g., data/pricing/informdata_costs.csv, data/pricing/competitors.csv).\n- Define exact CSV schemas listed above; document them in docs/sales/README.md. Reuse Task 3/11 schema/validator building blocks where applicable.\n\n2) Data preparation\n- Normalize content/pricing/informdata_pricing_table.csv into the canonical CSVs under data/pricing/ (costs, competitors, discounts, fees). Maintain numeric types for price/discounts and ISO 8601 dates.\n- Add lightweight data validation within scripts/generate_pricing_sheet.py (schema check, type coercion, bounds checks). Optionally, support reading the consolidated file as an interim source until full normalization is complete, but final generator must rely on the canonical CSVs.\n\n3) Template and content\n- Author templates/sales/pricing_sheet.md.j2 with variables for headline, value props, dynamic pricing tables, fees, competitor comparison, FAQs, CTA, and legal section include.\n- Extract and migrate content from the existing docs/sales/pricing_sheet.md into content/sales/informdata_pricing.yml and the template to preserve messaging and structure per the PRD.\n- Author content/sales/legal_disclaimer.md per the PRD tone and constraints.\n\n4) Generation pipeline\n- Implement scripts/generate_pricing_sheet.py:\n  - Load YAML and canonical CSVs, compute tier pricing, apply discounts, and format currency.\n  - Render templates/sales/pricing_sheet.md.j2 to docs/sales/informdata_pricing.md, superseding the manually authored draft.\n  - Convert MD to PDF (Pandoc or md-to-pdf). Include PDF metadata (Title: \"InformData Pricing\", Author: \"InformData\").\n  - Embed version/date stamp and data checksum in a hidden comment in the MD and in the PDF footer.\n- Add Makefile targets:\n  - pricing: create venv if missing, install deps, run generator, produce Markdown and PDF under the specified paths.\n  - clean: remove dist/sales/* and any temp build directories.\n\n5) CI and quality gates\n- Add .github/workflows/build-sales-collateral.yml:\n  - Triggers: push/PR on paths data/pricing/**, templates/sales/**, scripts/generate_pricing_sheet.py, content/sales/**, content/pricing/**.\n  - Steps: setup Python, install deps, run make pricing, run link checker on MD, upload PDF as artifact, fail on validation errors.\n- Add pre-commit hooks or CI steps for markdownlint, yamllint, and CSV schema validation.\n\n6) Accessibility and compliance\n- Ensure alt text is provided in YAML for each image; verify color usage for sufficient contrast in template.\n- Include legal disclaimer pulled from content/sales/legal_disclaimer.md; ensure all external links include UTM parameters if required by PRD.\n\n7) Documentation\n- Write docs/sales/README.md: data schema, update process (including how the consolidated dataset was normalized), how to run locally, CI description, and acceptance checklist referencing explicit repo paths. Link the Gamma deck as supplemental collateral.\n\nAcceptance criteria\n- The following files exist and are used by the generator: data/pricing/informdata_costs.csv, data/pricing/competitors.csv, data/pricing/discounts.csv, data/pricing/fees.csv, templates/sales/pricing_sheet.md.j2, scripts/generate_pricing_sheet.py, content/sales/informdata_pricing.yml, content/sales/legal_disclaimer.md.\n- Running `make pricing` produces docs/sales/informdata_pricing.md and dist/sales/informdata_pricing_{YYYYMMDD}.pdf without errors.\n- Generated Markdown includes the sections: Value Proposition, Pricing Tiers, Add-ons and Fees, Competitor Comparison, FAQs, Contact/CTA, and Legal.\n- CI workflow builds on PR and uploads the PDF artifact; lints and link checks pass.\n- docs/sales/README.md documents data schemas and the generation process clearly.\n- Existing draft docs/sales/pricing_sheet.md content is reflected in the generated docs/sales/informdata_pricing.md (migrated/ported) or clearly marked as superseded with a pointer to the generated output.",
        "testStrategy": "Local verification\n1) Environment setup\n- python -m venv .venv && source .venv/bin/activate (or .venv\\Scripts\\activate on Windows)\n- pip install -r requirements.txt\n\n2) Data validation and normalization\n- If present, validate content/pricing/informdata_pricing_table.csv loads and maps to canonical schemas.\n- Run: python scripts/generate_pricing_sheet.py --validate-only\n- Expect: success exit code; schema check reports 0 errors for CSV files at:\n  - data/pricing/informdata_costs.csv\n  - data/pricing/competitors.csv\n  - data/pricing/discounts.csv\n  - data/pricing/fees.csv\n\n3) Build outputs\n- Run: make pricing\n- Verify files exist:\n  - docs/sales/informdata_pricing.md\n  - dist/sales/informdata_pricing_YYYYMMDD.pdf (today’s date)\n- Open the Markdown and confirm presence of required headings: \"Value Proposition\", \"Pricing Tiers\", \"Add-ons and Fees\", \"Competitor Comparison\", \"FAQs\", \"Contact\", and \"Legal\".\n- Confirm that content from the previous draft docs/sales/pricing_sheet.md is represented in the generated MD (spot-check hero text, at least one pricing table, and FAQs), or that the draft file clearly points to the generated MD.\n- Inspect PDF metadata (Title == \"InformData Pricing\"). Verify size between 50 KB and 2 MB and all tables render without truncation across page boundaries.\n\n4) Lint and link checks\n- Run markdownlint on docs/sales/informdata_pricing.md (no errors).\n- Run a link checker (e.g., lychee or markdown-link-check) against docs/sales/informdata_pricing.md (no broken links). Validate that image paths under assets/images/sales/ resolve.\n\n5) Snapshot/content checks\n- Create a golden file tests/golden/informdata_pricing.md from the initial approved output.\n- Add a test that regenerates the MD and compares content while ignoring dynamic date/version lines via regex. Expect match.\n\n6) CI verification\n- Push a branch and open a PR touching data/pricing/**, content/pricing/**, and templates/sales/**.\n- Confirm .github/workflows/build-sales-collateral.yml runs, uploads the PDF artifact named informdata_pricing_YYYYMMDD.pdf, and enforces validation gates.\n\n7) PRD alignment\n- Cross-check against prd_sales_sheet.txt: verify each PRD-required section is present in the generated MD. Ensure competitor data and pricing tiers in the doc correspond to rows in data/pricing/competitors.csv and data/pricing/informdata_costs.csv.\n\n8) Reference collateral\n- Verify docs/sales/README.md includes a link to the Gamma deck for reference (no build dependency).\n\nPass criteria\n- All validations pass locally and in CI, outputs are generated at the specified repo paths, the consolidated dataset has been normalized or correctly mapped to canonical CSVs, and the content structure matches the PRD and Task 1 acceptance criteria.",
        "subtasks": [
          {
            "id": 1,
            "title": "Normalize consolidated pricing dataset into canonical CSVs",
            "description": "Read content/pricing/informdata_pricing_table.csv and produce data/pricing/informdata_costs.csv, data/pricing/competitors.csv, data/pricing/discounts.csv, and data/pricing/fees.csv per documented schemas. Preserve numeric types and ISO 8601 dates. Document mapping rules in docs/sales/README.md.",
            "dependencies": [],
            "details": "Implement a small transform within scripts/generate_pricing_sheet.py or a separate script (e.g., scripts/normalize_pricing_table.py). Use Task 3/11 validators to enforce schemas. Keep the consolidated CSV as a non-authoritative input until migration is complete.",
            "status": "pending",
            "testStrategy": "- Run the normalizer; then run the validator. Expect 0 schema errors. Spot-check a few SKUs for price and tier mapping integrity.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Migrate draft sales sheet content into YAML and template",
            "description": "Extract content from docs/sales/pricing_sheet.md into content/sales/informdata_pricing.yml and templates/sales/pricing_sheet.md.j2 to preserve tone/structure from the draft while enabling automated generation.",
            "dependencies": [
              1
            ],
            "details": "Map hero text, value props, FAQs, and CTA from the draft MD to YAML fields; move tables to be data-driven via Jinja using canonical CSVs.",
            "status": "pending",
            "testStrategy": "- Render the template to MD and diff against the draft for key sections (hero/value props/FAQs). Differences should be limited to formatting and dynamic data placeholders.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement generator and Makefile targets",
            "description": "Build scripts/generate_pricing_sheet.py to load YAML and canonical CSVs, render the MD, and produce the PDF. Add Makefile targets pricing and clean.",
            "dependencies": [
              1,
              2
            ],
            "details": "Include version/date stamp and data checksum. Ensure currency formatting and table pagination for PDF.",
            "status": "pending",
            "testStrategy": "- Run make pricing and verify outputs exist with correct metadata and size thresholds.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add CI workflow and quality gates",
            "description": "Create .github/workflows/build-sales-collateral.yml to build on PRs and main, run validations/lints, and upload PDF artifact.",
            "dependencies": [
              3
            ],
            "details": "Include markdownlint, yamllint, CSV schema validation, and link checking. Trigger on data/pricing/**, content/pricing/**, templates/sales/**, scripts/generate_pricing_sheet.py, content/sales/**.",
            "status": "pending",
            "testStrategy": "- Open a test PR modifying data and template files; confirm workflow passes and artifact is uploaded.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update docs and reference Gamma deck",
            "description": "Revise docs/sales/README.md to include schemas, normalization steps from the consolidated dataset, run instructions, CI description, and link to the Gamma deck.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Add acceptance checklist and clear pointers from the legacy draft MD to the generated MD.",
            "status": "pending",
            "testStrategy": "- Manually verify README contains required sections and the Gamma link resolves; run link checker to confirm no broken links.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T12:13:34.039Z"
      },
      {
        "id": "3",
        "title": "Implement InformData pricing data ingestion with schema and validation",
        "description": "Define a strict schema for the InformData pricing CSV and implement a Python ingestion and validation module plus a CLI to load, validate, and normalize the data for downstream use.",
        "details": "Scope and objectives\n- Create a canonical schema for InformData pricing data and enforce it at ingest time.\n- Provide a reusable Python module and a CLI that validates CSV files in data/pricing/, normalizes fields, and emits actionable error reports.\n- Align paths, naming, and conventions with the repo structure defined in Task 1 and the PRD (prd_sales_sheet.txt).\n\nRepository artifacts to add\n- schemas/pricing/informdata_costs.schema.json (authoritative column definitions and constraints; JSON Schema for documentation and CI checks)\n- src/pricing/ingest.py (Python ingestion/validation module)\n- scripts/validate_pricing_data.py (CLI wrapper around the ingestion validator)\n- tests/pricing/test_ingest.py (pytest coverage for valid and invalid cases)\n- tests/data/informdata_costs_valid.csv and tests/data/informdata_costs_invalid.csv (fixtures)\n- requirements.txt additions: pandas>=2.1, pandera>=0.17, python-dateutil>=2.8, typer>=0.9 (or argparse), tabulate>=0.9\n\nCSV schema (columns and constraints)\n- sku: string, required; pattern ^[A-Z0-9][A-Z0-9_\\-\\.]{1,63}$; trimmed; unique in combination with tier, unit, effective_date\n- description: string, required; non-empty; max length 256; trimmed\n- unit: enum, required; one of {per_search, per_report, per_record, monthly, one_time}\n- tier: integer, optional; >=1; default 1 if blank\n- min_qty: integer, optional; >=0; default 0\n- partner_cost: decimal, required; >=0\n- list_price: decimal, required; >=0\n- currency: string, required; ISO 4217 uppercase (e.g., USD, CAD); default USD\n- effective_date: date (YYYY-MM-DD), required\n- expire_date: date (YYYY-MM-DD), optional; must be >= effective_date when present\n- deprecated: boolean, optional; default false (accept y/n/true/false/1/0, normalized to true/false)\n- source: string, optional; URL or short code; max length 128\n\nTable-level constraints\n- Required header set must match exactly the required columns above; reject unknown extra columns unless explicitly whitelisted via a CLI flag.\n- Uniqueness: composite unique key (sku, tier, unit, effective_date). Duplicate rows produce an error with row numbers and the duplicate key values.\n- Consistency: list_price must be >= partner_cost. If equal, margin is 0; if list_price == 0 but partner_cost > 0, error.\n\nNormalization rules\n- Trim leading/trailing whitespace for all string fields; collapse internal repeated spaces in description.\n- Uppercase currency automatically; coerce dates to ISO 8601; coerce boolean variants to true/false.\n- Coerce numeric fields using Decimal-safe parsing; reject non-numeric with precise error messages.\n\nPython module (src/pricing/ingest.py)\n- Function: load_pricing_csv(path: str, strict: bool = true) -> pandas.DataFrame\n  - Reads CSV with explicit dtype hints; normalizes fields; validates against a Pandera SchemaModel; returns a clean DataFrame on success or raises a ValidationError with structured details.\n- Function: validate_pricing_csv(path: str, allow_extra_columns: bool = false) -> ValidationReport\n  - Runs full validation and returns a lightweight object with is_valid, errors (list of {row, column, code, message}), and summary stats.\n- Expose constants for REQUIRED_COLUMNS and ALLOWED_UNITS to centralize configuration.\n- Keep public APIs stable for Task 2 to import (e.g., from pricing.ingest import load_pricing_csv).\n\nCLI (scripts/validate_pricing_data.py)\n- Usage: python scripts/validate_pricing_data.py data/pricing/informdata_costs.csv [--format json|table] [--allow-extra-columns] [--fail-fast]\n- Exit codes: 0 on success, 2 on validation errors, 1 on unexpected exceptions.\n- Output: summary counts; first N errors; full JSON dump when --format json is used.\n\nJSON Schema (schemas/pricing/informdata_costs.schema.json)\n- Provide a documentation-grade schema mirroring the rules above (types, enum, patterns, required). This is used for human review and optional CI checks; runtime validation is performed by Pandera.\n\nError reporting and DX\n- Group errors by column with counts; include sample offending values and row numbers.\n- Print actionable remediation guidance (e.g., “currency must be a 3-letter ISO code like USD”).\n- Ensure deterministic ordering of errors for reproducibility.\n\nPerformance, quality, and CI hooks\n- Efficient read for 50k rows with validation under 5 seconds on a typical laptop (optimize dtypes and vectorized checks).\n- Add pytest cases: valid file passes; each rule has a failing example; duplicate composite key detection; boundary values for dates and numbers.\n- Optional pre-commit hook entry to run the validator on data/pricing/informdata_costs.csv when present.\n\nDocumentation\n- Update or add README notes in docs/ or module docstrings explaining schema columns, CLI usage, and integration point for Task 2’s generation pipeline.\n- Reference PRD (prd_sales_sheet.txt) to ensure column naming and semantics align with product messaging.\n",
        "testStrategy": "Environment setup\n1) Create and activate a virtual environment; install dependencies in requirements.txt (pandas, pandera, python-dateutil, typer/tabulate).\n\nSchema presence and contents\n2) Verify schemas/pricing/informdata_costs.schema.json exists and includes: required columns list, enum values for unit, regex pattern for sku, and ISO 8601 date format notes.\n\nUnit tests\n3) Run: pytest -q tests/pricing/test_ingest.py\n   - Expect: all tests pass. Confirm coverage of: required columns, enum checks, numeric bounds, date ordering, boolean coercion, currency normalization, duplicate key detection, and performance sanity (simple timing assertion).\n\nCLI positive path\n4) Place a valid sample at tests/data/informdata_costs_valid.csv. Execute: python scripts/validate_pricing_data.py tests/data/informdata_costs_valid.csv --format table\n   - Expect: “Validation OK” style summary, 0 errors, exit code 0.\n\nCLI negative path\n5) Execute: python scripts/validate_pricing_data.py tests/data/informdata_costs_invalid.csv --format json\n   - Expect: exit code 2. Output JSON includes keys is_valid=false and non-empty errors array with row, column, code, message. Verify at least one error each for: unknown column (when not using --allow-extra-columns), enum violation, numeric negative, currency not uppercase, date ordering, and duplicate composite key.\n\nNormalization checks\n6) Create a temporary CSV with lowercase currency, surrounding spaces in description, boolean variants (Y/0/True). Validate and assert the returned DataFrame from load_pricing_csv has normalized values: uppercase currency, trimmed description, booleans as true/false.\n\nPerformance sanity\n7) Generate a synthetic CSV of 10k rows (duplicate-free) and validate. Expect completion under 5 seconds on a typical dev machine and no memory errors.\n\nIntegration readiness\n8) From a Python REPL: from pricing.ingest import load_pricing_csv; df = load_pricing_csv('tests/data/informdata_costs_valid.csv'); assert len(df) > 0. This confirms the API surface for Task 2’s pipeline.\n",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Plan Task 3 implementation",
            "description": "Confirm schema mapping, extract InformData costs from finance workbook, and document validation steps.",
            "details": "<info added on 2025-10-30T11:48:03.970Z>\nPlan:\n1) Map finance workbook fields to base_costs schema\n- Create docs/mapping/informdata_finance_to_base_costs.md that links each finance workbook column/range to schemas/pricing/informdata_costs.schema.json fields, including:\n  - Service ID/SKU normalization rules (trim, uppercase, replace spaces/legacy delimiters, stable ID policy).\n  - Unit normalization map to canonical enums (e.g., hit, record, report, screen, monthly).\n  - Currency handling (USD), numeric parsing/rounding, effective_date parsing (yyyy-mm-dd), and null/default rules.\n  - Source provenance fields (source_sheet, source_version, workbook_path, checksum).\n- Capture any unmapped/derived fields and decisions.\n\n2) Build extraction script to normalize service IDs, units, and metadata\n- Implement scripts/pricing/extract_informdata_costs.py to read data/raw/informdata_finance_workbook.xlsx with a config file at config/pricing/informdata_extract.yaml (sheet names, header rows, column bindings).\n- Transformations:\n  - Normalize service IDs and SKUs per mapping, deduplicate, and flag collisions.\n  - Normalize units via mapping; validate against schema enum; record original_unit in metadata.\n  - Coerce currency to USD decimals, round to 4 dp; coerce dates to ISO-8601.\n  - Add vendor=InformData, source_version, effective_date, and provenance fields; compute source checksum.\n  - Filter discontinued rows or mark status where applicable; log all drops to logs/pricing/extract_informdata_costs.log.\n\n3) Generate data/pricing/informdata_costs.csv and retain transformation notebook/script for reproducibility\n- Script outputs data/pricing/informdata_costs.csv with columns ordered per schema.\n- Save an accompanying notebook at notebooks/pricing/informdata_costs_transform.ipynb documenting the same logic and sanity checks.\n- Emit provenance to data/pricing/provenance/informdata_costs_provenance.yaml (inputs, checksum, script version, run timestamp).\n- Add Makefile target: make informdata_costs to run the extract with the pinned config.\n\n4) Validate output with pricing validator and attach JSON report\n- Run: python scripts/validate_pricing.py --schema schemas/pricing/informdata_costs.schema.json --input data/pricing/informdata_costs.csv --report reports/validation/informdata_costs_validation.json --fail-on-error.\n- Attach reports/validation/informdata_costs_validation.json to this subtask and commit to repo.\n\n5) Document source approval reference in task notes\n- Add docs/approvals/informdata_source_approval.md capturing approver name/title, date, workbook version, and reference IDs/links (e.g., Jira FIN-####, email/Slack thread).\n- Mirror approver metadata in the provenance YAML and paste the approval reference into this subtask’s notes.\n\nAcceptance criteria\n- Mapping document exists and is reviewed; no unmapped required fields remain.\n- scripts/pricing/extract_informdata_costs.py reproducibly generates data/pricing/informdata_costs.csv from the finance workbook and config.\n- Validator report shows zero schema errors; any warnings are documented with rationale.\n- JSON validation report is attached; approval reference is recorded in task notes and docs/approvals.\n</info added on 2025-10-30T11:48:03.970Z>\n<info added on 2025-10-30T11:50:40.071Z>\nProgress update:\n- Extractor saved at scripts/pricing/extract_informdata_costs.py and run successfully.\n- Generated data/pricing/informdata_costs.csv with 22 services.\n- Added mapping doc at docs/data_schemas/mapping/informdata_finance_to_base_costs.md.\n- Ran pricing validator; validation reports saved under docs/data_schemas/reports/.\n\nFollow-ups to meet plan/acceptance:\n- Move/duplicate validation JSON to reports/validation/ and attach it to this subtask.\n- Align mapping doc location to docs/mapping/ or update plan to reflect current path.\n- Add Makefile target (make informdata_costs), emit provenance YAML, and commit the transform notebook.\n</info added on 2025-10-30T11:50:40.071Z>\n<info added on 2025-10-30T11:51:52.835Z>\nProgress update and plan/acceptance adjustments:\n- Validator reports will remain under docs/data_schemas/reports/ to keep artifacts co-located with schema docs; attach the JSON from this path to this subtask. Acceptance updated to permit this path instead of reports/validation/.\n- Mapping document is intentionally located at docs/data_schemas/mapping/; plan updated to reflect this path.\n- No Makefile target needed; repeatable extraction is provided via the CLI: python scripts/pricing/extract_informdata_costs.py --config config/pricing/informdata_extract.yaml. Acceptance updated to require CLI reproducibility rather than a Makefile target.\n</info added on 2025-10-30T11:51:52.835Z>\n<info added on 2025-10-30T11:53:24.734Z>\nProgress update:\n- CLI entry point now uses explicit --source and --output flags; no external config file is required.\n- Plan/comments updated to remove references to config/pricing/.\n\nPlan/acceptance adjustments:\n- Step 2 invocation: python scripts/pricing/extract_informdata_costs.py --source data/raw/informdata_finance_workbook.xlsx --output data/pricing/informdata_costs.csv\n- Acceptance: Reproducibility must be demonstrated via the flag-based CLI invocation above; no config/pricing/ files are required.\n</info added on 2025-10-30T11:53:24.734Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3,
            "updatedAt": "2025-10-30T11:53:39.839Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T11:53:39.839Z"
      },
      {
        "id": "4",
        "title": "Implement AI+SaaS pricing computation engine and persist outputs",
        "description": "Build a Python module and CLI that computes recommended AI+SaaS pricing per InformData SKU from validated base costs, AI usage, overheads, and target margins, and writes normalized outputs to CSV/JSON for downstream collateral.",
        "details": "Scope and objectives\n- Compute end-to-end unit economics and recommended prices for each InformData SKU by combining: base InformData cost, AI model usage cost, SaaS overheads, and target margin policy.\n- Persist machine-readable outputs for downstream use by the pricing collateral generator and other tools.\n- Align file paths, naming, and conventions with Task 1; consume validated inputs produced by Task 3.\n\nInputs and configuration (add to repo)\n- Input data (from Task 3):\n  - data/pricing/informdata_costs.csv (validated via schemas/pricing/informdata_costs.schema.json)\n- New assumptions and config files to add:\n  - data/pricing/usage_assumptions.csv (columns: sku, ai_calls_per_request, avg_input_tokens, avg_output_tokens)\n  - config/pricing/ai_costs.yaml (keys: model_name, input_per_1k_tokens_usd, output_per_1k_tokens_usd, per_call_fee_usd [optional])\n  - config/pricing/overheads.yaml (keys: infra_per_request_usd, support_overhead_pct_of_base, other_overhead_usd [optional])\n  - config/pricing/margins.yaml (keys: default_target_margin_pct, per_sku_overrides: { sku: pct })\n  - config/pricing/tiers.yaml [optional] (volume_breaks: list of {min_qty, discount_pct}; psychological_pricing: {enabled: true, ending: 0.99})\n- Output schemas (to be added for validation):\n  - schemas/pricing/ai_saas_pricing.schema.json (columns and constraints for computed outputs)\n\nOutput artifacts\n- data/pricing/ai_saas_pricing.csv\n- data/pricing/ai_saas_pricing.json\n- Optional: data/pricing/ai_saas_pricing.parquet (for analytics)\n\nOutput schema (csv/json rows)\n- sku (string)\n- base_cost_usd (number)\n- ai_cost_usd (number)\n- overhead_usd (number)\n- total_cost_usd (number)\n- target_margin_pct (number, 0-1)\n- recommended_unit_price_usd (number)\n- margin_dollars (number)\n- margin_pct (number, 0-1)\n- model_name (string)\n- notes [optional]\n- updated_at (ISO timestamp)\n- If tiers.yaml is provided, include tiered_pricing field in JSON (array of {min_qty, unit_price_usd, discount_pct}); CSV can include flattened best_unit_price_usd_at_1k, etc., or omit if not needed.\n\nComputation model\n- base_cost_usd: from data/pricing/informdata_costs.csv per sku.\n- ai_cost_usd per request:\n  - ai_tokens_cost = (avg_input_tokens/1000)*input_per_1k_tokens_usd + (avg_output_tokens/1000)*output_per_1k_tokens_usd\n  - per_call_total = ai_tokens_cost + per_call_fee_usd (default 0)\n  - ai_cost_usd = ai_calls_per_request * per_call_total\n- overhead_usd per request:\n  - overhead_usd = infra_per_request_usd + (support_overhead_pct_of_base * base_cost_usd) + other_overhead_usd (default 0)\n- total_cost_usd = base_cost_usd + ai_cost_usd + overhead_usd\n- target_margin_pct: per_sku_overrides.get(sku, default_target_margin_pct)\n- recommended_unit_price_usd:\n  - pre_round = total_cost_usd / (1 - target_margin_pct)\n  - if psychological_pricing.enabled: round up to nearest cent and set ending to configured value when >= 1.00; otherwise, round to 0.01\n- margin_dollars = recommended_unit_price_usd - total_cost_usd\n- margin_pct = margin_dollars / recommended_unit_price_usd\n- If tiers.yaml exists, apply discount_pct to recommended_unit_price_usd for each min_qty to produce tiered offers (never price below total_cost_usd; floor at total_cost_usd + $0.01)\n\nImplementation plan\n- Module: src/pricing/compute.py\n  - Functions: load_configs(), load_inputs(), compute_row(sku, base_row, assumptions, cfg), apply_psychological_pricing(price, ending), compute_tiers(price, tiers_cfg, total_cost), validate_output(df)\n  - Use pandas for data handling; ruamel.yaml or PyYAML for YAML; pandera for schema validation of outputs (align with schemas/pricing/ai_saas_pricing.schema.json)\n  - Ensure deterministic ordering by sku and stable rounding (quantize to 0.01)\n  - Emit logs with context (sku, components, totals)\n- CLI: scripts/compute_ai_saas_pricing.py (Typer-based)\n  - Arguments: --input, --assumptions, --ai-costs, --overheads, --margins, --tiers, --out-csv, --out-json, --out-parquet, --model-name override, --validate-only, --fail-on-below-margin\n  - Behavior: in validate-only mode, load and validate inputs/configs and dry-run compute with a few rows; in normal mode, compute all, validate output schema, write files\n- Schema: schemas/pricing/ai_saas_pricing.schema.json\n  - Define column types, non-negativity constraints, 0<=margin_pct<=1, recommended_unit_price_usd >= total_cost_usd + 0.01\n- Repo integration\n  - Makefile targets: make pricing-compute, make pricing-validate\n  - Document usage in AGENTS.md addendum and README snippet\n  - Ensure outputs are consumed by Task 2’s generation pipeline by referencing data/pricing/ai_saas_pricing.csv in scripts/generate_pricing_sheet.py\n\nData quality and edge cases\n- If a sku lacks usage_assumptions, skip with error unless --allow-missing is set; list missing in exit output\n- Guardrails to avoid negative or zero recommended prices\n- Support override of target margin per sku; log when overrides are applied\n- Timezone-aware timestamps (UTC)\n",
        "testStrategy": "Environment setup\n1) Create venv and install dependencies from requirements.txt (include pandas, pandera, pyyaml or ruamel.yaml, typer, pyarrow for parquet, python-dateutil).\n\nSmoke test with sample configs\n2) Seed minimal configs and assumptions:\n   - data/pricing/informdata_costs.csv: include at least 2 skus with base costs\n   - data/pricing/usage_assumptions.csv: include ai_calls_per_request, avg_input_tokens, avg_output_tokens per sku\n   - config/pricing/ai_costs.yaml: set model_name, input_per_1k_tokens_usd, output_per_1k_tokens_usd\n   - config/pricing/overheads.yaml: set infra_per_request_usd, support_overhead_pct_of_base\n   - config/pricing/margins.yaml: set default_target_margin_pct and one per_sku_overrides entry\n   - schemas/pricing/ai_saas_pricing.schema.json: present with constraints listed in Details\n\nValidation-only run\n3) Run: python scripts/compute_ai_saas_pricing.py --input data/pricing/informdata_costs.csv --assumptions data/pricing/usage_assumptions.csv --ai-costs config/pricing/ai_costs.yaml --overheads config/pricing/overheads.yaml --margins config/pricing/margins.yaml --validate-only\n   - Expect: exit code 0 and console output indicating configs and inputs validated; no output files created.\n\nFull compute run\n4) Run: python scripts/compute_ai_saas_pricing.py --input data/pricing/informdata_costs.csv --assumptions data/pricing/usage_assumptions.csv --ai-costs config/pricing/ai_costs.yaml --overheads config/pricing/overheads.yaml --margins config/pricing/margins.yaml --out-csv data/pricing/ai_saas_pricing.csv --out-json data/pricing/ai_saas_pricing.json\n   - Expect: files created at specified paths; JSON is a list of objects with required fields; CSV headers match output schema.\n\nSchema and invariants\n5) Validate outputs programmatically (pandera or JSON Schema):\n   - Columns present and correctly typed\n   - Non-negativity: base_cost_usd, ai_cost_usd, overhead_usd, total_cost_usd >= 0\n   - recommended_unit_price_usd >= total_cost_usd + 0.01\n   - 0 <= margin_pct <= 1 and recomputed margin matches within 0.005 tolerance\n   - model_name populated and consistent with config\n\nDeterminism and rounding\n6) Re-run the same command; compute hashes of output files and confirm equality (no nondeterministic variation). Spot-check prices end with .99 when psychological pricing enabled.\n\nError handling\n7) Remove a row from data/pricing/usage_assumptions.csv and re-run; expect non-zero exit with clear message listing missing sku unless --allow-missing is provided.\n\nIntegration readiness\n8) Open scripts/generate_pricing_sheet.py (Task 2 scope) and confirm it can read data/pricing/ai_saas_pricing.csv without code changes or with minor documented hook. Manually join with data/pricing/informdata_costs.csv and verify sku alignment and presence of recommended_unit_price_usd column.\n",
        "status": "done",
        "dependencies": [
          "1",
          "3"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Log deliverables",
            "description": "Documented compute script and validator reports for internal pricing.",
            "details": "<info added on 2025-10-30T12:25:12.072Z>\n- scripts/pricing/compute_internal_pricing.py generates data/pricing/internal_pricing.csv including a margin column.\n- Output validated against docs/data_schemas/reports/internal_pricing.json; schema checks pass.\n</info added on 2025-10-30T12:25:12.072Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4,
            "updatedAt": "2025-10-30T12:25:21.733Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T12:25:21.733Z"
      },
      {
        "id": "5",
        "title": "Research and record competitor MSRPs with verifiable evidence and provenance",
        "description": "Collect competitor MSRP/pricing data into a normalized CSV with rigorous evidence capture (screenshots/PDF/archive URLs), provenance metadata, and validation, aligned to the repo structure and PRD. Current progress exists in a non-canonical file (data/pricing/competitor_msps.csv) with citations to Checkr and GoodHire pricing pages and a validation report at docs/data_schemas/reports/competitor_msps.json. This task will migrate that progress into the canonical schema/paths, ensure evidence/provenance are complete, and keep the workflow auditable and reproducible.",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "high",
        "details": "Scope and objectives\n- Establish a repeatable, auditable workflow to research competitor MSRPs and persist them with evidence and provenance for inclusion in pricing collateral. Deliver a validated CSV plus evidence artifacts and a CLI to assist manual/assisted collection.\n- Integrate current progress: migrate data/pricing/competitor_msps.csv (contains Checkr and GoodHire citations) to canonical data/pricing/competitor_msrps.csv and produce matching provenance JSONL and changelog entries. Migrate/refresh the validation report to canonical path.\n- Follow repo conventions and paths defined in Task 1; do not scrape sites that disallow it. Prefer official pricing pages, product datasheets, and authorized resellers where permitted.\n\nRepository artifacts to add/maintain\n- Data and evidence\n  - data/pricing/competitor_msrps.csv (canonical dataset; migrate from existing data/pricing/competitor_msps.csv)\n  - data/pricing/competitor_msrps_provenance.jsonl (one JSON object per record with detailed capture metadata)\n  - data/evidence/competitors/<competitor>/<YYYYMMDD>/<slug>.(png|pdf|html.json) (page capture artifacts)\n  - data/pricing/competitor_msrps.changelog.md (human-readable change log)\n- Schemas and templates\n  - schemas/pricing/competitor_msrps.schema.json (authoritative JSON Schema; strict validation)\n  - templates/competitor_msrps_template.csv (empty, header-only CSV for manual entry)\n  - configs/competitors/sources.yaml (seed list of competitor pages and selectors; include Checkr and GoodHire entries with permitted flags)\n- Code\n  - scripts/collect_competitor_msrps.py (Typer CLI: fetch, add-manual, validate, verify-evidence, reconcile, import-legacy)\n  - scripts/utils/evidence.py (shared utilities for capture, hashing, archiving, robots checks)\n  - tests/test_competitor_msrps_validation.py (unit tests for schema and validation helpers)\n- Documentation\n  - docs/pricing/competitor_msrps.md (how-to, scope, compliance, assumptions, and current coverage including Checkr and GoodHire)\n  - docs/data_schemas/reports/competitor_msrps.json (canonical validation report; supersedes legacy docs/data_schemas/reports/competitor_msps.json)\n\nData model (columns in data/pricing/competitor_msrps.csv)\n- id: stable deterministic ID (sha1(competitor|product|plan|region|price_unit|source_url|effective_date))\n- competitor: string, required (normalized brand name)\n- product: string, required (product/SKU/plan family)\n- plan_tier: string (e.g., Basic/Pro/Enterprise), nullable\n- msrp_amount: decimal as string (e.g., 19.00), required\n- currency: ISO-4217 uppercase (e.g., USD, EUR), required\n- price_unit: enum [per_search, per_user_month, per_seat_month, per_org_month, per_report, per_batch, other]\n- region: enum [US, CA, EU, UK, AU, Global, Other]\n- min_commitment: string (e.g., monthly, annual, N/A)\n- included_units: integer or null\n- overage_price: decimal string or null\n- effective_date: YYYY-MM-DD (as stated by source if present; else match captured_at date)\n- source_url: URL string, required\n- source_title: string (page title at capture time)\n- archive_url: URL to a permanent archive (e.g., Wayback SavePageNow) if available\n- evidence_png: repo-relative path to PNG screenshot\n- evidence_pdf: repo-relative path to PDF print\n- evidence_hash_sha256: SHA-256 of concatenated artifact bytes (png||pdf) or of html.json when present\n- captured_at: ISO 8601 UTC timestamp\n- capture_method: enum [manual, assisted_playwright, third_party_archive]\n- tos_permits_capture: boolean (true if robots/TOS permit capture of listed page)\n- notes: free text (disclaimers, context, selector used)\n\nJSON Schema (schemas/pricing/competitor_msrps.schema.json)\n- Define types, required fields, enum constraints, formats (uri, date, date-time), and pattern for currency [A-Z]{3}.\n- Use minimum/maximum constraints where applicable (e.g., msrp_amount >= 0). Include $defs for enums and shared patterns.\n\nCLI implementation (scripts/collect_competitor_msrps.py)\n- Typer-based commands\n  - fetch: Read configs/competitors/sources.yaml and attempt assisted capture for permitted pages.\n    - For each entry: check robots.txt and TOS hints; if disallowed, skip and log.\n    - Use Playwright (Chromium) to: navigate, wait for networkidle, capture full-page screenshot (PNG) and PDF, extract page title, serialize visible DOM text into html.json.\n    - Compute sha256 over artifacts, store to evidence paths, call Archive.org SavePageNow API when allowed, and record archive_url.\n    - Parse price using CSS/XPath selectors if provided (BeautifulSoup/lxml fallback); never rely solely on parsing—operator must review.\n    - Write row to competitor_msrps.csv (append or update by id) and provenance to JSONL.\n  - add-manual: Open templates/competitor_msrps_template.csv or accept --row JSON to append a manually verified row; require evidence files and fill provenance.\n  - validate: Run schema validation (pandera or jsonschema) against competitor_msrps.csv; ensure evidence files exist on disk and hashes match.\n  - verify-evidence: Re-hash artifacts and confirm archive_url resolves (HEAD 200), warn if mismatch.\n  - reconcile: Deduplicate by id, prefer latest captured_at, and note changes in changelog.\n  - import-legacy: Ingest data/pricing/competitor_msps.csv (if present), map/rename fields to the canonical schema, compute IDs/hashes as needed, and write to competitor_msrps.csv with matching provenance JSONL; record actions in changelog and emit a validation report to docs/data_schemas/reports/competitor_msrps.json.\n\nEvidence and provenance best practices\n- Compliance-first: respect robots.txt, rate-limit (e.g., 1 req/sec, jitter), set descriptive User-Agent, and never bypass access controls. If disallowed, perform manual capture with user-initiated browser and attach evidence.\n- Immutable snapshots: store PNG and PDF; attempt SavePageNow; include both local paths and archive URL in CSV, with hashes stored in both CSV and provenance JSONL.\n- Reproducibility: keep configs/competitors/sources.yaml under version control with fields: competitor, product, plan_tier, region, url, selectors (css/xpath), permitted: true/false, notes. Seed entries must include Checkr and GoodHire.\n- Currency normalization: persist raw amounts and currency only; do not convert FX here. Downstream tasks may handle conversions.\n- Observability: log to logs/competitors/<date>.log with INFO/ERROR; include run metadata in provenance (script version, git commit).\n\nDependencies and alignment\n- Align file paths and naming with Task 1 conventions. Follow PRD requirements in prd_sales_sheet.txt regarding competitor context and how the data will be referenced by sales collateral.\n\nSecurity and maintainability\n- Pin dependencies in requirements.txt (playwright, pandas, pandera, typer, beautifulsoup4, lxml, requests, python-dateutil, pydantic, jsonschema). Add playwright install step in docs.\n- Use mypy/ruff config if present; keep functions pure and unit-testable; avoid embedding credentials. For Archive.org API, use environment variable ARCHIVE_ORG_SPN_API_KEY if applicable.\n",
        "testStrategy": "Environment setup\n1) python -m venv .venv && source .venv/bin/activate\n2) pip install -r requirements.txt && python -m playwright install chromium\n\nSchema and repo artifacts\n3) Confirm the following exist with correct paths and content:\n   - schemas/pricing/competitor_msrps.schema.json (open and verify required fields and enums)\n   - templates/competitor_msrps_template.csv (headers match schema)\n   - configs/competitors/sources.yaml (seed entries include Checkr and GoodHire with permitted flags and selectors)\n   - scripts/collect_competitor_msrps.py and scripts/utils/evidence.py\n   - docs/pricing/competitor_msrps.md\n\nImport existing progress and migrate to canonical\n4) If present, run: python scripts/collect_competitor_msrps.py import-legacy --src data/pricing/competitor_msps.csv --dst data/pricing/competitor_msrps.csv --provenance data/pricing/competitor_msrps_provenance.jsonl --report docs/data_schemas/reports/competitor_msrps.json\n   - Expect: canonical CSV created/updated; provenance JSONL appended; changelog updated; legacy validation report at docs/data_schemas/reports/competitor_msps.json is referenced in import notes.\n   - Verify: Checkr and GoodHire rows exist in the canonical CSV and reference evidence artifacts.\n\nAssisted/manual capture smoke test\n5) Run: python scripts/collect_competitor_msrps.py fetch --config configs/competitors/sources.yaml --out data/pricing/competitor_msrps.csv --evidence-dir data/evidence/competitors\n   - Expect: for permitted entries, PNG and PDF files created, html.json saved, archive_url populated when allowed, and rows appended to CSV with id populated.\n   - Verify: data/evidence/competitors/<competitor>/<YYYYMMDD>/ contains artifacts; the CSV evidence paths are repo-relative.\n\nValidation and evidence verification\n6) Run: python scripts/collect_competitor_msrps.py validate --csv data/pricing/competitor_msrps.csv --schema schemas/pricing/competitor_msrps.schema.json\n   - Expect: success exit code and printed summary (row count, 0 errors). If errors, they cite row numbers and field names.\n7) Run: python scripts/collect_competitor_msrps.py verify-evidence --csv data/pricing/competitor_msrps.csv\n   - Expect: all hashes recompute to the same value; archive URLs respond with HTTP 200 (or are marked N/A when saving not permitted).\n\nManual entry workflow\n8) Copy templates/competitor_msrps_template.csv to a working file and add at least 2 rows with real or test data; capture screenshots/PDFs manually and reference them. Use add-manual to append to the canonical CSV.\n   - Expect: rows are appended, ids computed, provenance JSONL gains entries, and validate still passes.\n\nData quality and reproducibility\n9) Run reconcile and confirm only a single row per unique id remains; changes are described in data/pricing/competitor_msrps.changelog.md.\n10) Spot check the Checkr and GoodHire rows: open evidence PNG/PDF and confirm the price, currency, and unit match the CSV exactly and that archive_url resolves.\n\nAutomation hygiene\n11) Run unit tests: pytest -q tests/test_competitor_msrps_validation.py\n   - Expect: all tests pass (schema loader, id generation, hash verification, robots checks).",
        "subtasks": [
          {
            "id": 1,
            "title": "Migrate legacy competitor_msps.csv to canonical competitor_msrps.csv with provenance",
            "description": "Implement and run a one-time migration from data/pricing/competitor_msps.csv to data/pricing/competitor_msrps.csv. Map/rename fields to match the schema, compute deterministic IDs, generate/append provenance JSONL, and update data/pricing/competitor_msrps.changelog.md. Do not delete the legacy file; reference it in changelog.",
            "dependencies": [],
            "details": "Use the new import-legacy CLI command. Ensure paths in evidence_* fields are repo-relative. Preserve source citations and notes. Emit docs/data_schemas/reports/competitor_msrps.json on completion.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Backfill and verify evidence for Checkr and GoodHire rows",
            "description": "Ensure PNG, PDF, and html.json artifacts exist for Checkr and GoodHire pricing pages, with sha256 hashes computed, archive_url saved (if permitted), and tos_permits_capture set correctly.",
            "dependencies": [],
            "details": "Store artifacts under data/evidence/competitors/<competitor>/<YYYYMMDD>/. Update provenance and CSV evidence fields accordingly. If robots/TOS disallow, document manual capture and set capture_method to manual.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Update validation report to canonical path and schema",
            "description": "Regenerate validation output and save to docs/data_schemas/reports/competitor_msrps.json. Cross-reference the legacy docs/data_schemas/reports/competitor_msps.json in docs/pricing/competitor_msrps.md.",
            "dependencies": [],
            "details": "Run CLI validate and persist a machine-readable report (row-level errors, summary). Ensure schema enums and formats are enforced.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Seed configs for Checkr and GoodHire in sources.yaml",
            "description": "Add curated entries for Checkr and GoodHire pricing pages to configs/competitors/sources.yaml with selectors and permitted flags.",
            "dependencies": [],
            "details": "Include competitor, product, plan_tier (if applicable), region, url, selectors (css/xpath), permitted: true/false, and notes. Validate via fetch smoke test.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add import-legacy command to CLI",
            "description": "Extend scripts/collect_competitor_msrps.py with an import-legacy Typer command to ingest data/pricing/competitor_msps.csv and write to canonical outputs.",
            "dependencies": [],
            "details": "Parameters: --src, --dst, --provenance, --report. Validate rows against schema, compute IDs, reconcile duplicates, and write a summary to stdout and changelog.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T12:16:41.161Z"
      },
      {
        "id": "6",
        "title": "Execute competitor MSRP research and populate validated dataset with evidence",
        "description": "Perform hands-on research to capture competitor MSRPs, archive verifiable evidence (screenshots/PDF/archive URLs), and populate the normalized CSV and provenance metadata defined in the repo. Initial competitive research for Checkr and GoodHire has been completed, documented with evidence, tied to the pricing table, and synced to docs/sales/pricing_sheet.md and the Gamma sales enablement deck.",
        "status": "done",
        "dependencies": [
          "5"
        ],
        "priority": "high",
        "details": "Scope and objectives\n- Execute the actual data collection for competitor MSRPs using the schema, CLI, and workflow from Task 5. Produce a complete, validated dataset with rigorous evidence and provenance for use in pricing collateral.\n- Adhere to repo conventions defined by Task 5 (which aligns to Task 1) and to the schema at schemas/pricing/competitor_msrps.schema.json.\n- Current progress: Completed competitive research for Checkr and GoodHire with documented MSRPs tied to the pricing table; outputs synced to docs/sales/pricing_sheet.md and reflected in the Gamma sales enablement deck.\n\nDeliverables\n- data/pricing/competitor_msrps.csv populated with current MSRPs for targeted competitors/products/SKUs (includes completed rows for Checkr and GoodHire).\n- data/pricing/competitor_msrps.provenance.jsonl containing one JSON object per row with evidence/provenance metadata (includes Checkr and GoodHire entries with evidence).\n- evidence/pricing/competitors/<vendor>/<YYYY-MM-DD>/ containing screenshots (PNG) and print-to-PDF captures for each source, plus an optional raw HTML snapshot if supported by the CLI.\n- docs/pricing/RESEARCH_NOTES.md summarizing scope, date range, sources used, ambiguities/exclusions, and mapping decisions.\n- docs/sales/pricing_sheet.md updated to include or reference a \"Competitor MSRPs\" section/table derived from data/pricing/competitor_msrps.csv; ensure alignment with pricing table terminology and SKU mapping.\n- Sales enablement Gamma deck updated to reflect the latest competitor MSRP figures and citations (slide(s) show source URLs and evidence timestamps), staying consistent with docs/sales/pricing_sheet.md.\n\nTarget coverage\n- Use prd_sales_sheet.txt and any companion target list (if provided) to define the initial scope of vendors/products/SKUs. If no list exists, propose one in RESEARCH_NOTES.md and get approval before collection.\n- Checkr and GoodHire are already covered; continue with remaining approved targets.\n- Prioritize official pricing pages, authorized resellers (when official MSRP is publicly unavailable), and product datasheets. Do NOT scrape sites that disallow it; respect robots.txt and site terms.\n\nData fields to capture (align to schema)\n- vendor, product_name, sku (or closest mapping), msrp_value (numeric), currency (ISO 4217), billing_period (e.g., monthly, annual, per-lookup), tier (if applicable), region (default US unless otherwise specified), last_verified_at (ISO8601 UTC), source_url, archive_url(s), evidence_paths (list), evidence_hashes (sha256 per file), notes (e.g., exclusions, promo caveats), collector (your initials).\n\nResearch workflow\n1) Environment prep\n- Activate the project venv and install dependencies (Playwright/Chromium per Task 5). Ensure the Task 5 CLI is available (e.g., scripts/competitor_msrp.py or equivalent).\n\n2) Plan the run\n- Draft a target checklist: vendor → product/SKU → primary source URL → fallback source(s). Store in docs/pricing/RESEARCH_NOTES.md.\n- Confirm permissibility: check robots.txt and terms for each site; if disallowed, skip and document.\n\n3) Collect prices and evidence\n- For each target:\n  - Navigate to the source URL in Chromium. Ensure region/currency is US/USD when applicable.\n  - Capture both a full-page PNG screenshot and a print-to-PDF using the Task 5 CLI (or Playwright). Suggested path pattern: evidence/pricing/competitors/<vendor>/<YYYY-MM-DD>/<vendor>__<product>__<timestamp>.{png,pdf}\n  - Compute sha256 for each evidence file; store in provenance JSON.\n  - Save an archive URL (web.archive.org and/or archive.today) when permitted; record timestamp.\n  - Extract MSRP verbatim. If price is tiered, record the base MSRP and note tiers; if only range is available, record min/max and note the range.\n  - Capture billing period and any qualifiers (e.g., “per seat”, “per lookup”, minimum commitments, annual prepay). Document promo pricing as promo, not MSRP.\n\n4) Normalize and record\n- Use the Task 5 validation/ingest CLI to add or update rows in data/pricing/competitor_msrps.csv and data/pricing/competitor_msrps.provenance.jsonl.\n- Ensure currency codes are ISO and msrp_value is numeric. Do not convert currencies unless the CLI supports it; otherwise record native currency and note it.\n- De-duplicate by composite key (vendor+product+sku+billing_period+region). If multiple valid sources exist, prefer official source and keep alternates in provenance JSON as additional_sources.\n\n5) Quality checks\n- Run the CLI validator to enforce schemas and cross-check that every CSV row has corresponding evidence files and hashes.\n- Manually spot-check at least 20% of rows: open evidence and confirm the price and qualifiers match the CSV exactly.\n\n6) Documentation and commit hygiene\n- Summarize the collection window, sources, and any open questions in docs/pricing/RESEARCH_NOTES.md. Note that Checkr and GoodHire have been completed and tied to the pricing table.\n- Commit CSV, provenance JSONL, and evidence files with meaningful messages. Avoid committing large binaries unnecessarily; if LFS is configured, ensure evidence files are tracked by LFS.\n- Sync and verify alignment with docs/sales/pricing_sheet.md and the Gamma deck to ensure figures, qualifiers, and timestamps match the dataset and evidence.\n\nEdge cases and guidance\n- If MSRP is not publicly listed: record msrp_value as empty, include evidence of non-availability, and add notes with reasoning. If an authorized reseller publishes MSRP, use that with a clear note.\n- If prices vary by region: prefer US; otherwise record the region and keep each region as a separate row.\n- If dynamic pricing obscures MSRP: capture the configuration that produced the price (e.g., seats=10, term=annual) in notes and provenance.\n- Maintain auditability: every CSV row must be reproducible from provenance (URL, archive URL, evidence files, hashes, timestamp).",
        "testStrategy": "Environment and tooling\n1) Activate venv and ensure Task 5 CLI is installed/available; playwright install chromium completed.\n\nSchema and structural validation\n2) Confirm files exist:\n   - data/pricing/competitor_msrps.csv\n   - data/pricing/competitor_msrps.provenance.jsonl\n   - evidence/pricing/competitors/<vendor>/<YYYY-MM-DD>/...\n3) Run the Task 5 validator (example):\n   - python scripts/competitor_msrp.py validate --csv data/pricing/competitor_msrps.csv --provenance data/pricing/competitor_msrps.provenance.jsonl\n   Expect: success exit code; no schema violations.\n\nEvidence integrity\n4) Run the evidence check (example):\n   - python scripts/competitor_msrp.py check-evidence --csv data/pricing/competitor_msrps.csv --provenance data/pricing/competitor_msrps.provenance.jsonl\n   Expect: every row references existing evidence files; computed sha256 matches stored values; archive URLs resolve (HTTP 200) for at least one archival service per row.\n\nSpot checks\n5) Randomly sample at least 20% of rows:\n   - Open PNG/PDF evidence and confirm the displayed price, currency, billing period, and qualifiers match the CSV exactly.\n   - Follow source_url and archive_url; confirm content consistency with evidence timestamp.\n\nCompetitor coverage sanity\n6) Verify that data/pricing/competitor_msrps.csv contains rows for Checkr and GoodHire with correct fields populated; provenance entries exist and point to valid evidence and archives.\n\nCompleteness and cross-document consistency\n7) Compare the final CSV against the approved target list in docs/pricing/RESEARCH_NOTES.md:\n   - 100% of agreed vendors/products/SKUs are present or explicitly documented as unavailable with evidence of non-availability.\n8) Open docs/sales/pricing_sheet.md:\n   - Contains a \"Competitor MSRPs\" section/table that matches the dataset for Checkr and GoodHire (values, currency, billing period, qualifiers, and notes) and any additional completed vendors.\n   - Cross-check that terminology/SKU mapping aligns with the pricing table.\n9) Gamma deck review:\n   - Slides referencing competitor MSRPs reflect the same figures and qualifiers as the CSV and evidence; citations include source URL and verification date.\n\nDocumentation\n10) Open docs/pricing/RESEARCH_NOTES.md:\n   - Contains collection window, scope, sources used, known gaps, decisions, and approved target list; explicitly notes completion for Checkr and GoodHire.\n\nVersion control hygiene\n11) Run git lfs ls-files (if LFS is used) to ensure large evidence binaries are tracked. Commit history shows a clear, auditable trail.",
        "subtasks": [
          {
            "id": 1,
            "title": "Finalize and validate Checkr MSRP entries with evidence and provenance",
            "description": "Add/confirm Checkr rows in data/pricing/competitor_msrps.csv with correct fields; ensure matching objects exist in data/pricing/competitor_msrps.provenance.jsonl; include PNG/PDF evidence, sha256 hashes, and archive URLs.",
            "dependencies": [],
            "details": "Confirm region=US (unless otherwise noted), currency=USD, accurate billing period and qualifiers. Ensure evidence stored under evidence/pricing/competitors/Checkr/<YYYY-MM-DD>/ and linked in provenance.",
            "status": "done",
            "testStrategy": "Run validator and evidence checks; manually open Checkr evidence files and confirm CSV alignment. Verify Checkr appears correctly in docs/sales/pricing_sheet.md and Gamma deck.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Finalize and validate GoodHire MSRP entries with evidence and provenance",
            "description": "Add/confirm GoodHire rows in data/pricing/competitor_msrps.csv with correct fields; ensure matching objects exist in data/pricing/competitor_msrps.provenance.jsonl; include PNG/PDF evidence, sha256 hashes, and archive URLs.",
            "dependencies": [],
            "details": "Confirm region=US (unless otherwise noted), currency=USD, accurate billing period and qualifiers. Ensure evidence stored under evidence/pricing/competitors/GoodHire/<YYYY-MM-DD>/ and linked in provenance.",
            "status": "done",
            "testStrategy": "Run validator and evidence checks; manually open GoodHire evidence files and confirm CSV alignment. Verify GoodHire appears correctly in docs/sales/pricing_sheet.md and Gamma deck.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Sync competitor MSRP dataset to docs/sales/pricing_sheet.md and Gamma deck",
            "description": "Ensure the \"Competitor MSRPs\" table/section in docs/sales/pricing_sheet.md and the Gamma deck reflect the dataset (values, qualifiers, timestamps, citations) and pricing table mappings.",
            "dependencies": [],
            "details": "If discrepancies are found, update docs/sales/pricing_sheet.md and adjust slides to match the dataset and evidence timestamps. Record changes in commit messages.",
            "status": "done",
            "testStrategy": "Diff dataset vs. docs table and slides; confirm exact matches for Checkr and GoodHire rows. Validate links/citations open and correspond to evidence timestamps.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Author and commit docs/pricing/RESEARCH_NOTES.md including approved target list",
            "description": "Summarize scope, collection window, sources used, decisions, ambiguities, and include a proposed/approved target list beyond Checkr and GoodHire.",
            "dependencies": [],
            "details": "Explicitly note completion status for Checkr and GoodHire and any follow-ups required for remaining vendors. Include robots.txt considerations and exclusions.",
            "status": "pending",
            "testStrategy": "Open docs/pricing/RESEARCH_NOTES.md and verify presence of all required sections and target list. Cross-reference with dataset coverage.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Collect, evidence, and record remaining approved competitor MSRPs",
            "description": "Execute the research workflow for remaining vendors/products/SKUs, capturing evidence and provenance, and updating CSV/JSONL per schema.",
            "dependencies": [],
            "details": "Follow the established path patterns, hashing, archival, and normalization rules. Prefer official sources; document unavailability with evidence.",
            "status": "pending",
            "testStrategy": "Run validation and evidence checks after each batch; spot-check ≥20% of new rows. Ensure deduplication by composite key.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Run validators and ensure version control hygiene (incl. LFS)",
            "description": "Validate schemas, evidence integrity, and ensure large binaries are tracked with Git LFS. Commit with meaningful messages and cross-references to docs updates.",
            "dependencies": [],
            "details": "Use scripts/competitor_msrp.py validate and check-evidence commands. Verify archive URLs resolve and hashes match.",
            "status": "pending",
            "testStrategy": "Validator exits successfully; evidence checks pass; git lfs ls-files shows PNG/PDF evidence; commit history is clear and auditable.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T12:19:28.116Z"
      },
      {
        "id": "7",
        "title": "Draft InformData pricing sales collateral using computed tables and competitor insights",
        "description": "Author a draft Markdown pricing sheet that merges pricing computation outputs with competitor MSRP insights, generating derived tables and a Jinja-based template to produce a cohesive sales collateral document.",
        "details": "Scope and objectives\n- Create a first-pass, publishable Markdown draft of the InformData pricing sales collateral that uses: (a) computed pricing outputs from Task 4 and (b) validated competitor MSRPs from Task 6.\n- Produce derived data tables (CSV/JSON) for the collateral and a Jinja template to render the final Markdown draft. Do not build the full automated pipeline (that is Task 2). This task focuses on content, derived tables, and a simple render script for previewing.\n\nInputs\n- PRD: prd_sales_sheet.txt (messaging, required sections).\n- Computed pricing output (from Task 4): outputs/pricing/recommended_prices.csv (and/or JSON) with columns including sku, rec_price, unit_cost, ai_usage_cost, overhead, target_margin, gross_margin_pct (exact names from Task 4 output; script should be configurable).\n- Competitor dataset (from Task 6): data/pricing/competitor_msrps.csv and data/pricing/competitor_msrps.provenance.jsonl plus evidence/pricing/ files.\n\nArtifacts to add\n- templates/collateral/pricing_sheet.md.j2 (Jinja template for the sales collateral).\n- scripts/generate_collateral_tables.py (merges Task 4 outputs and competitor MSRPs; computes derived tables and context JSON).\n- scripts/render_pricing_collateral.py (renders Jinja template to Markdown using derived context).\n- artifacts/pricing/tables/ (generated):\n  - sku_price_ladder.csv (sku, description, unit_cost, ai_usage_cost, overhead, rec_price, target_margin, gross_margin_pct).\n  - competitor_comparison.csv (sku, our_rec_price, competitor, competitor_price, price_diff, savings_pct, notes, provenance_ref).\n  - value_index.csv (sku, competitor, our_rec_price, competitor_price, value_index = competitor_price/our_rec_price).\n- artifacts/pricing/context.json (aggregated context for templating: summary stats, date stamps, footnotes/citations, table paths).\n- docs/sales/informdata_pricing_sheet.md (rendered draft for review).\n- Optional helper mapping if needed: data/pricing/sku_competitor_map.csv (sku, competitor, competitor_sku) to align SKUs; only create if a direct join is not possible.\n\nImplementation details\n1) Outline and content planning\n- From prd_sales_sheet.txt, define major sections in the Jinja template: Executive summary, Offering overview, Pricing and tiers (computed tables), Competitive comparison, Price/margin rationale (unit economics), Objection handling/FAQs, Call-to-action, Disclaimers and evidence.\n- Include structured placeholders/macros for reusable blocks (e.g., pricing_disclaimer, evidence_footnotes()).\n\n2) Derived tables and context generation (scripts/generate_collateral_tables.py)\n- Read recommended prices from Task 4 outputs (path via CLI args; default: outputs/pricing/recommended_prices.csv).\n- Read competitor MSRPs from data/pricing/competitor_msrps.csv and provenance JSONL.\n- Join datasets on sku (or via data/pricing/sku_competitor_map.csv if direct join ambiguous). Document unmatched rows; include them in sku_price_ladder.csv and omit from competitor tables with a reason code.\n- Compute metrics:\n  - price_diff = our_rec_price - competitor_price\n  - savings_pct = (competitor_price - our_rec_price) / competitor_price\n  - value_index = competitor_price / our_rec_price\n- Generate artifacts/pricing/tables/*.csv and artifacts/pricing/context.json including:\n  - summary: total_skus, matched_skus, median_value_index, date_generated (UTC), data_sources (file paths and git commit if available).\n  - citations: map provenance_ref -> list of evidence items (URL, archived_url, snapshot path, captured_at).\n- Validate numeric fields (non-negative prices), ensure percentage bounds (-1.0 to 10.0 to tolerate edge cases), and round for display while preserving raw values in context.\n\n3) Template authoring (templates/collateral/pricing_sheet.md.j2)\n- Render tables using context/tables; example patterns:\n  - A formatted price ladder per SKU showing costs, target margin, and recommended price.\n  - A competitor comparison table with footnote markers [^ref] linking to evidence in the Disclaimers/Evidence section.\n- Include clear copy blocks: positioning statements, benefits, assumptions (currency, date-of-capture), and a note on dynamic pricing updates.\n- Add graceful handling for missing competitor data (show N/A and note “no verified MSRP available as of {{ date }}”).\n\n4) Simple rendering script (scripts/render_pricing_collateral.py)\n- CLI args: --template templates/collateral/pricing_sheet.md.j2 --context artifacts/pricing/context.json --out docs/sales/informdata_pricing_sheet.md\n- Load Jinja2, render with context, and write the Markdown. Ensure idempotent execution so Task 2 can later wrap it.\n\n5) Style and conventions\n- Align paths and naming with Task 1 conventions. Keep scripts documented with --help. Include docstrings and type hints.\n- Do not add a PDF generator here; markdown-only output for review. Task 2 will formalize the build.\n\nNon-goals\n- Computing pricing (Task 4) and collecting competitor data (Task 6) are inputs, not reimplemented here.\n- Establishing the full CI/automation build (Task 2) is out of scope here.\n",
        "testStrategy": "Prerequisites\n- Ensure Task 4 and Task 6 are complete and their outputs exist.\n- python -m venv .venv && source .venv/bin/activate\n- pip install -r requirements.txt (include pandas, jinja2, typer/tabulate or click, python-dateutil).\n\nValidation steps\n1) Input presence and schema sanity\n- Confirm files exist:\n  - outputs/pricing/recommended_prices.csv (from Task 4)\n  - data/pricing/competitor_msrps.csv and data/pricing/competitor_msrps.provenance.jsonl (from Task 6)\n- If applicable, create data/pricing/sku_competitor_map.csv and verify required columns exist.\n\n2) Generate derived tables\n- Run: python scripts/generate_collateral_tables.py \\\n    --prices outputs/pricing/recommended_prices.csv \\\n    --competitors data/pricing/competitor_msrps.csv \\\n    --provenance data/pricing/competitor_msrps.provenance.jsonl \\\n    --out-dir artifacts/pricing/tables \\\n    --context-out artifacts/pricing/context.json \\\n    --fail-on-unmatched 0\n- Expect: exit code 0; files created:\n  - artifacts/pricing/tables/sku_price_ladder.csv\n  - artifacts/pricing/tables/competitor_comparison.csv\n  - artifacts/pricing/tables/value_index.csv\n  - artifacts/pricing/context.json\n- Spot check row counts: sku_price_ladder rows == number of SKUs in recommended_prices.csv; competitor_comparison matched_skus <= total_skus, never > total_skus.\n- Consistency checks:\n  - For each SKU, recompute gross_margin_pct from unit_cost/overhead/rec_price and verify it equals the value from Task 4 within 0.1pp tolerance.\n  - For each competitor row, ensure provenance_ref maps to at least one evidence item in provenance JSONL.\n\n3) Render the Markdown draft\n- Run: python scripts/render_pricing_collateral.py \\\n    --template templates/collateral/pricing_sheet.md.j2 \\\n    --context artifacts/pricing/context.json \\\n    --out docs/sales/informdata_pricing_sheet.md\n- Expect: docs/sales/informdata_pricing_sheet.md exists and includes sections: Executive summary, Pricing and tiers, Competitive comparison, Evidence/Disclaimers.\n- Validate that all Jinja placeholders are resolved (no remaining {{ ... }} or {% ... %}).\n\n4) Content and numeric QA\n- Randomly pick 3 SKUs; verify rec_price and gross_margin_pct in the Markdown match Task 4 outputs.\n- Verify competitor comparison rows show correct calculations for price_diff and savings_pct.\n- Confirm each competitor entry with a footnote [^ref] resolves to a provenance entry listing URL or archived URL.\n\n5) Lint and idempotency\n- Re-run steps 2 and 3; confirm outputs update deterministically (no duplicated rows, stable ordering by sku asc then competitor asc).\n- Optional: run simple unit tests if provided under tests/ for table computations; all pass.\n",
        "status": "done",
        "dependencies": [
          "1",
          "4",
          "6"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Document collateral outputs",
            "description": "Link Markdown pricing sheet and Gamma presentation.",
            "details": "<info added on 2025-10-30T12:36:49.540Z>\n- Published Markdown pricing sheet: docs/sales/pricing_sheet.md\n- Generated Gamma sales deck for the sales team: https://gamma.app/docs/zdlm8b8chaqwk0y\n</info added on 2025-10-30T12:36:49.540Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-10-30T12:56:57.126Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T12:56:57.126Z"
      },
      {
        "id": "8",
        "title": "Execute finance/legal review and approvals for InformData pricing collateral",
        "description": "Coordinate and complete formal Finance and Legal review of the pricing collateral draft, capturing approvals and gating publication via repository workflows. Produce a review packet, run checklists, record approver metadata, and finalize an approved release of the collateral.",
        "details": "Scope and objectives\n- Run an end-to-end approval process for the InformData pricing collateral produced in Task 7, ensuring compliance with pricing policy and legal standards. Gate merges/publishing on explicit Finance and Legal approvals captured in-repo.\n\nInputs and artifacts (expected from prior tasks)\n- Draft collateral (Markdown/PDF) and derived tables from Task 7 (e.g., docs/collateral/pricing/informdata_pricing_draft.md and a rendered PDF under build/collateral/).\n- Computed pricing outputs (CSV/JSON) and configs from Task 4 (consumed indirectly via Task 7’s draft).\n- Competitor MSRPs dataset and evidence from Task 6 (referenced by Task 7).\n- PRD: prd_sales_sheet.txt.\n\nDeliverables\n- Review packet under docs/review/packets/pricing_collateral_vX.Y/ containing:\n  - Draft Markdown + PDF of the collateral.\n  - Data extracts used in the draft (derived tables, recommended pricing CSV/JSON).\n  - Competitor MSRPs CSV and a provenance summary linking to evidence/.\n  - A review memo (docs/review/memos/pricing_collateral_review_memo.md) summarizing assumptions, margin policy, disclaimers, and change log.\n- Checklists:\n  - docs/review/checklists/finance_checklist.md (margin floors, discount guardrails, rounding, FX assumptions, effective dates, price presentation, SKU mapping).\n  - docs/review/checklists/legal_checklist.md (claims substantiation, competitive comparisons, trademarks/brand usage, licensing/IP, regulatory/region notes, required disclaimers/footnotes, evidence archiving).\n- Approval metadata file: docs/review/approvals/pricing_collateral_approval.yaml capturing: document_version, commit_sha, collateral_paths, finance_approver, legal_approver, approval_dates, notes, and links to packet.\n- Optional automation to enforce approvals:\n  - CODEOWNERS entry requiring Finance and Legal on docs/collateral/pricing/** and docs/review/**.\n  - GitHub Action (.github/workflows/review_metadata.yml) invoking scripts/review/validate_approvals.py to fail if approval YAML is missing/invalid or CODEOWNERS approvals aren’t present.\n- Final approved collateral with DRAFT watermark removed and a versioned release tag (e.g., collateral/pricing/v1.0-approved).\n\nImplementation steps\n1) Prepare review packet\n- Assemble all inputs into docs/review/packets/pricing_collateral_vX.Y/; include a README explaining contents and how to reproduce the draft.\n- Create the review memo summarizing pricing methodology, key assumptions, material risks, and deltas from prior versions.\n- Add references to specific data files and evidence folders used by the draft.\n\n2) Author and run checklists\n- Fill out finance_checklist.md and legal_checklist.md against the draft and packet contents.\n- Fix any high-priority findings in the draft or data; for substantive pricing/model changes, route updates back to Task 4/Task 7 and iterate until checklists pass.\n\n3) Capture approvals and enforce gates\n- Add/confirm CODEOWNERS entries that require Finance and Legal for changes to collateral and review folders.\n- Create docs/review/approvals/pricing_collateral_approval.yaml and populate fields with approvers’ names, titles, dates, and commit SHA.\n- Open a PR titled \"Approval: InformData pricing collateral vX.Y\" attaching the packet path and memo. Request reviews from Finance and Legal.\n- If using the optional Action, commit scripts/review/validate_approvals.py and .github/workflows/review_metadata.yml to validate presence and schema of the approval YAML and to check for required reviewers.\n\n4) Finalize approved collateral\n- After approvals, remove any DRAFT watermark or labels from the Markdown/PDF, update STATUS/metadata (e.g., docs/collateral/pricing/STATUS.md -> Approved), and bake required disclaimers/footnotes into the final document.\n- Tag the repo with collateral/pricing/vX.Y-approved and create a GitHub Release attaching the approved PDF and a zip of the review packet.\n- Archive the signed approval YAML and packet; ensure evidence links remain accessible.\n\nConstraints and considerations\n- Do not publish or distribute beyond the repo until both Finance and Legal approvals are recorded.\n- Maintain provenance of all claims; no unsubstantiated competitor statements.\n- Keep PRD alignment; any deviations must be documented in the review memo.\n- Minimize scope creep: editorial fixes are fine; pricing or methodology changes must loop back to the owning tasks.\n",
        "testStrategy": "Prerequisites\n- Ensure Task 7 artifacts exist (draft Markdown/PDF and derived tables).\n\nValidation steps\n1) Packet assembly\n- Confirm docs/review/packets/pricing_collateral_vX.Y/ contains: draft Markdown, PDF, derived tables, pricing outputs, competitor MSRPs CSV, provenance summary, and review memo.\n- Open all links in the memo to verify evidence/provenance is reachable under evidence/pricing/.\n\n2) Checklist completion\n- Verify finance_checklist.md and legal_checklist.md are present, filled, and committed.\n- Ensure any checklist issues have corresponding commits addressing them; no unresolved high/medium severity items remain.\n\n3) Approval enforcement\n- Open a PR modifying collateral and review files and observe CODEOWNERS requests reviewers from Finance and Legal.\n- Without approvals and without a valid docs/review/approvals/pricing_collateral_approval.yaml, ensure the PR cannot be merged (branch protection + required status check if the Action is configured).\n- Add a minimally valid approval YAML with all required fields and obtain two approvals; verify the status check passes and the PR becomes mergeable.\n\n4) Finalization and release\n- After merge, confirm docs/collateral/pricing/STATUS.md indicates Approved and the PDF no longer has any DRAFT markers.\n- Verify a git tag collateral/pricing/vX.Y-approved exists and a GitHub Release includes the approved PDF and a zip of the review packet.\n- Spot-check that disclaimers/footnotes required by Legal are present in the final document.\n\n5) Auditability\n- Run scripts/review/validate_approvals.py locally against docs/review/approvals/pricing_collateral_approval.yaml; expect a success exit code.\n- Confirm the commit SHA in the approval YAML matches the approved collateral commit and that approver names/titles/dates are populated.",
        "status": "pending",
        "dependencies": [
          "7"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "9",
        "title": "Package approved pricing collateral for Gamma export and sales-ready artifacts",
        "description": "Build a packaging tool and workflow that converts the approved pricing collateral into Gamma-compatible Markdown plus PDF/PPTX one-pagers, generates a zipped import bundle, and publishes sales-ready artifacts with versioned manifests.",
        "details": "Scope and objectives\n- Take the Finance/Legal-approved collateral from Task 8 and generate a versioned, sales-ready deliverable set: Gamma-compatible Markdown bundle, a PDF one-pager, a short slide deck (PPTX), and a manifest with approvals and checksums.\n- Preserve approver metadata from Task 8, stamp the collateral with version and approval data, and place artifacts under build/collateral/ with a stable structure for Sales enablement and releases.\n\nInputs and expected locations (from Task 8)\n- Approved collateral Markdown/PDF and derived tables: docs/review/packets/pricing_collateral_vX.Y/ (contains the final approved Markdown and a rendered PDF from Task 8).\n- Pricing outputs and competitor MSRPs dataset referenced by Task 7/8 (e.g., data/pricing/competitor_msrps.csv, derived tables under build/collateral/derived/).\n- Approval metadata (JSON/MD) produced in Task 8: docs/review/packets/pricing_collateral_vX.Y/approvals.json (or equivalent) with approver names, roles, timestamps.\n\nRepository changes\n- Add CLI and packaging module:\n  - tools/collateral/package_gamma.py (Typer-based CLI).\n  - tools/collateral/gamma_lint.py (basic rules to check Markdown compatibility: no raw HTML blocks, max header depth H3, valid relative asset paths, no footnotes; auto-convert where possible).\n- Config and templates:\n  - configs/collateral/export.yaml (brand colors, logo path, PPTX template path, one-pager options, title/section ordering, output toggles).\n  - templates/collateral/one_pager.jinja.md (condensed one-pager view rendered from approved data).\n  - templates/pptx/brand_template.potx (placeholder brand template; allow override via config).\n- Output tree (versioned):\n  - build/collateral/gamma/vX.Y/\n    - manifest.json (version, git commit, approvals, source hashes, file list + checksums)\n    - markdown/informdata_pricing_approved.md (Gamma-friendly Markdown)\n    - images/ (exported figures/logos referenced by Markdown)\n    - tables/ (CSV/JSON tables referenced by the Markdown)\n    - pdf/ (informdata_pricing_one_pager.pdf, informdata_pricing_full.pdf)\n    - pptx/ (informdata_pricing_deck.pptx)\n    - zip/gamma_import_bundle_vX.Y.zip (bundle of markdown, images, tables, manifest)\n\nCLI behavior (package_gamma.py)\n- Commands\n  - package: Build all artifacts. Args: --version vX.Y --approvals <path> --input-md <path> --derived <dir> --outdir <dir> --config <path> --brand-logo <path> --pptx-template <path> --skip-pdf/--skip-pptx.\n  - lint: Run Gamma-compatibility checks on Markdown and assets; can auto-fix simple issues (convert footnotes, flatten link refs, convert HTML tables to Markdown tables, cap header depth, rewrite image paths).\n  - manifest: Generate manifest.json with SHA256 checksums, file sizes, git commit, approvals metadata, and source file hashes.\n- Transformations\n  - Read the approved Markdown from Task 8; run gamma_lint to enforce rules and perform safe auto-fixes. Write the cleaned Markdown to markdown/informdata_pricing_approved.md.\n  - Resolve and copy all referenced images/assets to images/ and tables/; rewrite relative links accordingly.\n  - One-pager: Render templates/collateral/one_pager.jinja.md using approved data/derived tables, then convert to PDF (pandoc/pypandoc if available; fallback to reportlab/simple PDF renderer). Place in pdf/.\n  - Full PDF: If Task 8 already produced an approved PDF, copy into pdf/; otherwise render from approved Markdown via pandoc if available.\n  - PPTX: Use python-pptx with templates/pptx/brand_template.potx to generate a concise deck (title, value prop, pricing summary/table, competitive positioning). Insert brand colors/logo from config.\n  - Bundle: Create zip/gamma_import_bundle_vX.Y.zip containing markdown/, images/, tables/, and manifest.json.\n  - Manifest: Aggregate approvals (names, roles, timestamps), version, git commit, input file hashes, outputs with SHA256 checksums, and a reproducibility section (command line, config snapshot).\n\nImplementation notes\n- Keep Gamma Markdown simple: headings ≤ H3, standard Markdown tables, no raw HTML; inline callouts with blockquotes; ensure image widths are not specified via HTML.\n- Prefer CSV tables for large data; link in Markdown with brief excerpts. Keep image files under 2 MB each where possible (Pillow to downscale/compress PNG/JPEG).\n- Brand assets (logo, colors) configurable in configs/collateral/export.yaml; fail gracefully if missing and proceed with defaults.\n- Do not modify any pricing numbers or text content beyond formatting/linting; content remains as approved in Task 8.\n- Add Makefile targets: make package-gamma VERSION=vX.Y and make clean-gamma.\n- requirements.txt additions: typer, jinja2, python-pptx, pillow, pypandoc (optional), reportlab (fallback), python-dateutil, PyYAML.\n\nAcceptance criteria\n- A versioned directory under build/collateral/gamma/vX.Y/ exists with the files listed above.\n- gamma_import_bundle_vX.Y.zip imports cleanly into Gamma via Markdown import (no broken links, images render, tables intact) and passes internal lint with zero errors.\n- PPTX and one-pager PDF are generated and branded per config (or gracefully skipped with clear logs if optional dependencies are unavailable).\n- manifest.json includes approvals metadata from Task 8 and checksums for all outputs.\n- Makefile targets run end-to-end locally and in CI, attaching artifacts to a GitHub Release when a version tag is pushed (add a minimal CI workflow if one exists).",
        "testStrategy": "Prerequisites\n- Ensure Task 8 is complete and locate: docs/review/packets/pricing_collateral_vX.Y/ (approved Markdown, approved PDF if available, approvals.json) and derived tables referenced by Task 7/8.\n- python -m venv .venv && source .venv/bin/activate\n- pip install -r requirements.txt\n\nFunctional verification\n1) Lint and auto-fix\n- Run: python tools/collateral/package_gamma.py lint --input-md docs/review/packets/pricing_collateral_vX.Y/informdata_pricing_approved.md --outdir build/collateral/gamma/vX.Y --config configs/collateral/export.yaml\n- Confirm output markdown/informdata_pricing_approved.md exists and lint report shows 0 errors (warnings acceptable but ≤ threshold 3).\n\n2) Package all artifacts\n- Run: python tools/collateral/package_gamma.py package --version vX.Y --approvals docs/review/packets/pricing_collateral_vX.Y/approvals.json --input-md docs/review/packets/pricing_collateral_vX.Y/informdata_pricing_approved.md --derived build/collateral/derived --outdir build/collateral/gamma/vX.Y --config configs/collateral/export.yaml --brand-logo docs/assets/brand/logo.png --pptx-template templates/pptx/brand_template.potx\n- Verify files exist:\n  - build/collateral/gamma/vX.Y/manifest.json\n  - markdown/informdata_pricing_approved.md\n  - images/ (non-empty)\n  - tables/ (matches referenced tables)\n  - zip/gamma_import_bundle_vX.Y.zip\n  - pptx/informdata_pricing_deck.pptx (or clear log explaining skip)\n  - pdf/informdata_pricing_one_pager.pdf and pdf/informdata_pricing_full.pdf (or clear log explaining skip)\n\n3) Manifest integrity\n- Open manifest.json and confirm: version vX.Y, approvals list populated, git commit present, SHA256 checksums for all output files.\n- Compute a checksum for one file (e.g., shasum -a 256 markdown/informdata_pricing_approved.md) and verify it matches manifest.\n\n4) Gamma import sanity (manual)\n- In Gamma, import the zip/gamma_import_bundle_vX.Y.zip or the Markdown file with images/tables folder.\n- Confirm: headings render correctly, all images display, tables are formatted, no raw HTML artifacts. Record pass/fail in a short QA note under docs/collateral/pricing/releases/QA_vX.Y.md.\n\n5) PPTX and PDF checks\n- Open pptx/informdata_pricing_deck.pptx and verify: title slide, pricing summary slide with table, competitive positioning slide; brand logo present if provided.\n- Open pdf/informdata_pricing_one_pager.pdf; confirm sections are present and data matches the approved Markdown (spot-check a few values).\n\n6) CI/release attachment (if CI present)\n- Tag a test release (e.g., vX.Y-rc1) and push; verify the CI job publishes build/collateral/gamma/vX.Y artifacts to the GitHub Release and that manifest.json is included.\n\n7) Idempotency\n- Re-run the package command; confirm outputs are identical (checksum match) when inputs/config unchanged.\n",
        "status": "done",
        "dependencies": [
          "8"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Unify NatCrim UI + counts + tokens + header partials",
            "description": "Clean up and unify the language and structure across NatCrim pages; clarify database numbers (distinct databases vs dataset listings), remove confusing metrics, and improve quick lists UX with pill pills; integrate InformData marketing copy where appropriate.",
            "details": "Deliverables\n- styles/tokens.css with color, type, spacing tokens; import in 4 pages.\n- components/header.html partial and lightweight include script; cross-links: Overview, Components, Coverage, Fulfillment, Directory.\n- Refactors in national_scan_components.html, databases_coverage.html, statewide_vs_county.html, informdata_source_list.html to use tokens and shared header; align counts and labels.\n- Copy updates: \"Fulfillment methods\" naming, remove \"0 states without coverage\", clarify \"Statewide covers county requirements\" vs \"Covered via county research only\"; simple info alert block.\n- Accessibility: focus outline and color contrast tokens.\n- Documentation: docs/notes/design_system.md capturing tokens and screenshots (placeholder).",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9,
            "updatedAt": "2025-11-04T18:45:20.254Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-11-04T18:45:20.254Z"
      },
      {
        "id": "10",
        "title": "Generate collateral-ready pricing tables from approved InformData costs and margin policy",
        "description": "Build a tool that filters approved InformData costs, applies the margin policy via the pricing engine, and emits collateral-ready pricing tables in CSV/JSON/YAML for downstream Markdown/PDF generation.",
        "details": "Scope and objectives\n- Read validated InformData cost data and select only approved, in-force rows as of a given date.\n- Apply target margins using the pricing computation engine (Task 4) to produce SKU-level list prices and related derived fields suitable for sales collateral.\n- Emit machine-readable and human-consumable tables plus a manifest for provenance and reproducibility.\n\nInputs and assumptions\n- Costs input: data/pricing/informdata_costs.csv (validated by Task 3). The schema must include columns such as: sku, sku_name, description, unit, cost_basis, approval_status, effective_date, end_date (nullable), currency, category, notes. approval_status should include values like approved, draft, deprecated.\n- Pricing policy/config: configs/pricing/policy.yaml (margin targets, rounding rules, currency handling) and optional configs/pricing/rounding.yaml.\n- Pricing engine: import and use the public API produced in Task 4 (e.g., pricing.engine.compute(df_costs, policy) or equivalent CLI/module), not re-implementing pricing logic.\n\nArtifacts to add\n- Module: src/pricing/collateral_tables.py (functions to load, filter, compute, and export tables).\n- CLI: scripts/build_collateral_pricing_tables.py with options: --as-of-date YYYY-MM-DD, --policy PATH, --output-dir PATH, --region, --channel, --currency, --include-skus, --exclude-skus, --rounding-profile, --strict.\n- Output tables (created on run):\n  - derived/pricing/collateral/informdata_pricing_tables.csv\n  - derived/pricing/collateral/informdata_pricing_tables.json\n  - derived/pricing/collateral/informdata_pricing_tables.yaml (Jinja-friendly context for collateral rendering)\n  - derived/pricing/collateral/manifest.json (inputs, checksums, git SHA, as_of_date, policy version, engine version)\n- Output schema: schemas/pricing/collateral_pricing_tables.schema.json defining required columns and constraints.\n\nOutput columns (minimum)\n- sku, sku_name, category, unit\n- cost_basis (numeric), target_margin_pct (numeric)\n- computed_price (numeric, pre-round), list_price (numeric, post-round), currency\n- effective_date, end_date (nullable)\n- channel, region (nullable), notes (nullable)\n- source_cost_version (e.g., hash or version string), policy_version, engine_version\n\nFiltering and computation rules\n- Only include rows where approval_status == \"approved\" and effective_date <= as_of_date and (end_date is null or end_date >= as_of_date).\n- If multiple approved rows exist for the same SKU, choose the one with the most recent effective_date not after as_of_date; error if overlapping effective windows unless --allow-overlap is set (optional future flag).\n- Pass normalized costs to Task 4 pricing engine along with policy for margin and any AI/SaaS overheads defined therein. Do not duplicate pricing logic here; this task orchestrates and shapes collateral-ready outputs.\n- Apply rounding rules from policy/rounding profiles (e.g., 0.99 endings, currency minor units). Validate rounded values remain >= computed_price and respect minimums.\n- Support optional filters: by region/channel and SKU allow/deny lists.\n\nImplementation notes\n- Use pandas and pandera for IO/validation; typer for CLI; jsonschema for output schema; python-dateutil for dates.\n- Implement a small adapter to call the Task 4 engine. Example shape:\n  from pricing.engine import compute_prices\n  df_prices = compute_prices(df_costs=approved_df, policy=policy_cfg)\n- Build a deterministic export: stable column order, sorted by category then sku.\n- Write a manifest with SHA256 checksums of inputs (costs CSV, policy files), output files, git commit, timestamp, and as_of_date.\n- Document usage in README.md section: how to run the CLI and where outputs land.\n\nError handling and logging\n- Fail fast if input schema invalid (reuse Task 3 validator if available).\n- Emit clear errors for missing policy keys, unknown currencies, or overlapping cost windows.\n- Log summary: #SKUs in, #SKUs approved, #SKUs priced, #errors, min/median/max margins.\n\nAcceptance criteria\n- Running the CLI with a valid as_of_date produces all outputs under derived/pricing/collateral/ matching the output schema and including only approved SKUs.\n- Prices are computed by Task 4 engine and match the engine’s outputs (modulo rounding), with policy/rounding versions stamped in manifest.json.\n- YAML output is loadable as a Jinja context for downstream collateral rendering (Task 7 will consume it).",
        "testStrategy": "Prereqs\n- Ensure Task 3 and Task 4 are complete and available in the environment.\n- Create and activate a virtualenv; pip install -r requirements.txt.\n\nData and config setup\n1) Validate costs input\n- Run the Task 3 validator against data/pricing/informdata_costs.csv and confirm success.\n\n2) Prepare policy\n- Confirm configs/pricing/policy.yaml exists with target margins and rounding profile; adjust as needed for the test.\n\nFunctional test\n3) Run the CLI\n- python scripts/build_collateral_pricing_tables.py --as-of-date 2025-01-01 --policy configs/pricing/policy.yaml --output-dir derived/pricing/collateral\n- Expect exit code 0.\n\n4) File existence and structure\n- Confirm the following exist:\n  - derived/pricing/collateral/informdata_pricing_tables.csv/json/yaml\n  - derived/pricing/collateral/manifest.json\n  - schemas/pricing/collateral_pricing_tables.schema.json\n\n5) Schema validation\n- Use a jsonschema/pandera check to validate informdata_pricing_tables.json/csv against schemas/pricing/collateral_pricing_tables.schema.json.\n\n6) Content checks\n- Confirm only rows with approval_status == approved (from input) are present and effective_date windows include the as_of_date.\n- Spot-check a few SKUs: recompute through the Task 4 engine or compare against engine outputs to ensure computed_price matches pre-round values and list_price reflects rounding rules.\n- Verify currency and rounding to minor units are correct; list_price >= computed_price.\n\n7) Determinism\n- Re-run the CLI with identical inputs; diff outputs should show no changes.\n\n8) Manifest integrity\n- Open derived/pricing/collateral/manifest.json and confirm it contains: input file paths and SHA256 checksums, git SHA, as_of_date, engine version, policy version, and checksums for output files.\n\n9) Error-path tests\n- Change a row’s approval_status to draft and re-run; confirm the SKU is excluded.\n- Create an overlapping effective window for a SKU and re-run with --strict (default); expect a non-zero exit and a clear error message.\n\n10) Jinja context sanity\n- In a Python REPL: load the YAML output and access a few fields (e.g., sku, list_price) to ensure it’s a clean mapping suitable for templating.",
        "status": "in-progress",
        "dependencies": [
          "3",
          "4"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Align collateral branding with Vuplicity",
            "description": "Replace residual Booplicity mentions in generated pricing outputs and collateral, regenerate tables, and ensure UI copy clearly distinguishes InformData vendor costs from Vuplicity automation/platform allocations.",
            "details": "<info added on 2025-10-31T01:47:48.976Z>\nPlan and execution details\n\n1) Audit for legacy names and guidance\n- Scan code, templates, docs, and generated assets for residual Booplicity/Gamma references:\n  - rg -n \"Booplicity|booplicity\" --hidden --glob '!venv/**' --glob '!build/**' .\n  - rg -n \"Gamma|gamma\\.?com\" --hidden --glob '!venv/**' --glob '!build/**' .\n- Expected allowed Gamma references: only under task_9 packaging code and docs (see Acceptance Criteria). All others must be removed or updated.\n- Update brand constants/placeholders:\n  - templates, Jinja vars, and context builders should use brand_name = \"Vuplicity\".\n  - Replace logo paths to assets/branding/vuplicity/* and remove assets/branding/booplicity/* if unused.\n- Touchpoints to review/update:\n  - templates/collateral/*.j2, templates/email/*.j2\n  - tools/pricing/*.py, tools/collateral/*.py\n  - docs/collateral/*, docs/deploy/*, README.md\n  - .taskmaster/tasks/*.json and task descriptions\n  - build scripts and Makefile/Taskfile if present\n\n2) Regenerate pricing CSV/HTML with Vuplicity messaging and clarified copy\n- Ensure inputs and engine are valid:\n  - python tools/pricing/validate_costs.py --input data/pricing/informdata_costs.csv\n  - python tools/pricing/compute_prices.py --costs data/pricing/informdata_costs.csv --as-of today --out build/pricing/vuplicity/\n- Emit CSV/JSON/YAML plus manifest:\n  - python tools/pricing/build_pricing_tables.py --in build/pricing/vuplicity/derived.json --formats csv json yaml --out build/collateral/tables/\n  - Confirm manifest at build/collateral/tables/manifest.json (includes source commit, schema versions, as_of date).\n- Render HTML collateral with updated copy:\n  - Update templates/collateral/pricing_vuplicity.html.j2 to include the following content model:\n    - Headline: “Vuplicity pricing for InformData-sourced searches”\n    - Columns: “InformData vendor cost (pass-through)”, “Vuplicity automation/platform add-ons”, “Total MSRP”\n    - Footnotes:\n      1) “InformData vendor cost is a direct pass-through and may change per vendor notice.”\n      2) “Vuplicity automation/platform add-ons cover orchestration, retries, monitoring, reporting, and SLA tooling; they are configurable.”\n      3) “Total MSRP = vendor cost + selected add-ons; volume tiers and contractual discounts may apply.”\n    - Badge/tagline: “Powered by Vuplicity automation and platform controls”\n  - Render:\n    - python tools/collateral/render_pricing_html.py --tables build/collateral/tables/ --template templates/collateral/pricing_vuplicity.html.j2 --out build/collateral/pricing_vuplicity.html\n\n3) Update docs and deployment notes; rerun builders to keep outputs in sync\n- Remove or rewrite stale Gamma guidance in:\n  - docs/collateral/branding.md (replace Booplicity examples with Vuplicity)\n  - docs/deploy/pricing_builders.md (current build steps; no Gamma-specific content unless referencing Task 9)\n  - docs/gamma.md (either delete if obsolete for this flow or replace with a pointer: “Gamma packaging is handled by Task 9”)\n- Restate deployment notes:\n  - Environment: python -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt\n  - Commands: run validate_costs, compute_prices, build_pricing_tables, render_pricing_html as above\n  - Outputs: build/collateral/tables/* and build/collateral/pricing_vuplicity.html are the authoritative artifacts for Sales\n- Rerun the full builder sequence and commit updated outputs with a single version bump in build/collateral/tables/manifest.json.\n\nAcceptance criteria (DoD)\n- Zero occurrences of “Booplicity” in repo and generated outputs:\n  - rg -n \"Booplicity|booplicity\" --hidden --glob '!venv/**' . returns no matches.\n- “Gamma” appears only in Task 9 code/docs:\n  - Allowed: task_9 source files under tools/packaging/gamma/* and docs/packaging/gamma.md (or equivalent), plus Task 9 manifest templates.\n  - All other Gamma references removed or replaced with Task 9 pointer.\n- Updated HTML collateral present at build/collateral/pricing_vuplicity.html with:\n  - Correct headline, columns, and the three footnotes above.\n- Tables emitted at build/collateral/tables/ in CSV, JSON, and YAML, with a manifest containing source commit, as_of date, and schema versions.\n- Docs updated: branding, deployment notes, and Gamma guidance reconciled; links resolve; examples show Vuplicity only.\n- Pricing recomputed from approved InformData costs (validator passes) and matches the engine outputs for the same as_of date.\n\nNotes for reviewers\n- Verify that “InformData vendor cost (pass-through)” is labeled exactly and appears in column headers and legend.\n- Confirm add-on descriptions enumerate orchestration, retries, monitoring, reporting, and SLA tooling.\n- Confirm logo and brand colors are from assets/branding/vuplicity/ and no Booplicity assets remain.\n</info added on 2025-10-31T01:47:48.976Z>\n<info added on 2025-10-31T03:09:22.170Z>\nProgress update\n\n- Regenerated internal pricing and consolidated tables with Vuplicity automation/platform assumptions; emitted CSV/JSON/YAML plus updated manifest under build/collateral/tables/ (as_of and source commit populated).\n- Published matching JSON used by HTML dashboards and wired templates to consume it during render_pricing_html.\n- Swapped branding and UI copy across pricing breakout, county fee, and national scan pages:\n  - Column headers: “InformData vendor cost (pass-through)”, “Vuplicity automation/platform add-ons”, “Total MSRP”.\n  - Footnotes (1–3) and “Powered by Vuplicity automation and platform controls” badge included.\n  - Logos/colors now sourced from assets/branding/vuplicity/.\n- Scrubbed Gamma references from docs and PRD; retained only Task 9 packaging pointers. Updated collateral instructions to reflect current builder workflow.\n- Verification:\n  - rg for “Booplicity|booplicity” returns no matches.\n  - rg for “Gamma|gamma\\.?com” limited to allowed Task 9 packaging paths.\n  - HTML renders and dashboards load the new JSON; callouts explicitly distinguish InformData vs. Vuplicity costs.\n- Next: finalize commit with single version bump in build/collateral/tables/manifest.json and request reviewer sign-off.\n</info added on 2025-10-31T03:09:22.170Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10,
            "updatedAt": "2025-10-31T03:09:53.271Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Publish collateral updates and new Vercel pages",
            "description": "Deploy refreshed pricing breakout plus dedicated court fee and database coverage pages, ensuring Vercel site renders each asset with Vuplicity messaging.",
            "details": "<info added on 2025-11-02T23:14:21.914Z>\nImplementation plan and deliverables\n\n1) County fee page\n- Source data: copy latest county fee output into public/data/county_fees.json (fallback: convert CSV to JSON with scripts/tools/csv_to_json.py).\n- Page: pages/court-fees.html (static) with client-side filtering (no server code). Include:\n  - Filters: State (multi-select), County (typeahead), Court type (if present in data), Fee range, “Only show surcharges/notes” toggle.\n  - Results table with sticky header, virtualized rendering for large lists, CSV export of filtered view.\n  - Provenance banner and Vercel deployment guidance section (explain client-side JSON fetch, no serverless required, cache headers).\n  - Vuplicity messaging block and brand styles.\n- JS module: public/js/court_fees.js to handle fetch(/data/county_fees.json), filters, URL state (query params), and accessibility.\n- Build hook: add script to sync latest data to public/data on build (e.g., npm run sync-data:fees).\n\n2) Databases coverage page\n- Source: docs/downloads.md (and linked downloads) → normalize into public/data/database_coverage.json via scripts/coverage/extract_downloads.py.\n- Page: pages/database-coverage.html with:\n  - Summary cards (products covered, jurisdictions count, last refresh date).\n  - Coverage table: product, jurisdiction scope, coverage %, last updated, source URL, known limitations, provenance id.\n  - “How to interpret coverage” section for reps, with explicit data provenance, refresh cadence, and caveats.\n  - Link to data dictionary and download of the JSON/CSV snapshot.\n  - Vuplicity messaging block.\n- JS module: public/js/database_coverage.js for client-side rendering, filtering, sorting.\n\n3) Wire into site and deploy\n- Navigation: update partials/header.html and partials/footer.html to add “Court Fees” and “Database Coverage” links; ensure existing Pricing Breakout link remains visible.\n- Pricing breakout: verify pages/pricing-breakout.html reads from public/data/pricing_breakout.json (refresh data during build).\n- SEO: update public/sitemap.xml and add noindex to any internal-only staging variants if applicable.\n- Local sanity check:\n  - vercel pull\n  - vercel dev (or npm run dev) and open /court-fees and /database-coverage\n  - Validate filters, export buttons, mobile layout, and that JSON loads with 200.\n- Build and deploy:\n  - npm run build (ensures data sync tasks run)\n  - vercel build\n  - vercel deploy --prebuilt --prod\n- Post-deploy validation: open production URLs, confirm cache-control headers on JSON, check console for errors, run basic Lighthouse pass.\n\nAcceptance criteria\n- Court Fees page loads latest data from public/data/county_fees.json; filters work, export functions, and guidance/provenance text is present; styling matches Vuplicity.\n- Database Coverage page renders from public/data/database_coverage.json; includes provenance, interpretation guidance, and working filters/sorts.\n- Header/footer include links to Court Fees, Database Coverage, and Pricing Breakout; sitemap includes both new pages.\n- Local dev and production builds succeed; site deployed via Vercel CLI with no console errors; JSON assets are cacheable but bust on redeploy.\n- All content is client-rendered (no serverless functions), accessible (keyboard/tab, aria labels), and mobile-responsive.\n</info added on 2025-11-02T23:14:21.914Z>\n<info added on 2025-11-02T23:21:18.146Z>\nProgress\n- Copied the standalone Court Fee dashboard and National Scan components into the repository root for Vercel static hosting.\n- Added databases_coverage.html at the root; page renders from content/pricing/national_scan_sources.json (generated from the InformData workbook).\n- Refreshed navigation across all pages to include the new collateral links and preserve Pricing Breakout.\n- Committed (feat: expand collateral pages (task 10)) and pushed to GitHub.\n- Deployed to Vercel production (deploy id: 6gczsKVuuVzn7ZdrftTMhv7W39Cq).\n\nFollow-ups to align with plan and acceptance criteria\n- Rename databases_coverage.html to database-coverage.html (hyphen) and add to public/sitemap.xml.\n- Normalize data path: emit public/data/database_coverage.json (via a transform script) or update the JS fetch to the new source; include downloadable JSON/CSV snapshot and provenance/refresh cadence on the page.\n- Verify Court Fees page sources public/data/county_fees.json; confirm filters, CSV export, accessibility, and Vuplicity messaging are present.\n- Ensure Database Coverage page includes interpretation guidance, provenance IDs, known limitations, data dictionary link, and working filters/sorts.\n- Confirm header/footer retain the Pricing Breakout link and that pages/pricing-breakout.html reads from public/data/pricing_breakout.json.\n- Set cache-control headers on JSON and confirm cache bust on redeploy; run Lighthouse and console checks on production.\n- Add/verify build sync tasks (e.g., npm run sync-data:fees and coverage data sync) and ensure client-only rendering (no serverless).\n</info added on 2025-11-02T23:21:18.146Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10,
            "updatedAt": "2025-11-02T23:21:27.937Z",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Expose full InformData source list",
            "description": "Transform the InformData National Criminal workbook into a detailed dataset and publish an interactive page for Sales to browse coverage by state, jurisdiction, and record type.",
            "details": "<info added on 2025-11-03T00:59:50.539Z>\nScope and deliverables\n- Normalize the InformData National Criminal workbook into a JSON dataset with fields: state, jurisdiction, source_name, record_type, coverage_notes, updated_at (ISO 8601).\n- Build an interactive HTML page for browsing/filtering/searching/exporting the dataset.\n- Wire the page into site navigation, update docs, commit, and redeploy to Vercel.\n\nImplementation plan\n1) ETL and validation\n- Input: data/sources/informdata_natcrim.xlsx\n- Script: scripts/parse_natcrim_sources.py (Typer CLI: parse, validate)\n- Output: data/coverage/informdata_natcrim.json\n- Schema: schemas/coverage/informdata_natcrim.schema.json (types, required fields, enums where applicable)\n- Parsing rules:\n  - Trim whitespace; normalize state to USPS codes; unify jurisdiction casing; coalesce merged cells; preserve multiline coverage_notes.\n  - Map workbook date fields to updated_at in UTC ISO 8601 (YYYY-MM-DD).\n  - Drop empty rows; dedupe exact duplicates (state+jurisdiction+source_name+record_type).\n- Makefile targets:\n  - make natcrim-parse (generate JSON)\n  - make natcrim-validate (run schema validation)\n- Tests: add tests/test_natcrim_parser.py with sample fixtures covering edge cases (merged cells, missing dates, multiline notes).\n\n2) Interactive page\n- Files: site/coverage/informdata_sources.html, site/coverage/informdata_sources.css, site/coverage/informdata_sources.js\n- Data load: fetch /data/coverage/informdata_natcrim.json (static asset served on Vercel)\n- UX requirements:\n  - Filters: multi-select State; text search on jurisdiction and source_name; checkboxes for record_type; free-text search across coverage_notes.\n  - Results table: sortable columns, sticky header, client-side pagination; row count badge.\n  - Export: buttons to download current filtered view as CSV and JSON; include applied filters in export filename.\n  - Shareable links: encode current filters in URL query params for deep-linking.\n  - Accessibility: keyboard navigation, ARIA labels, color-contrast compliant.\n  - Performance: initial render ≤ 200ms on 5k rows; filter interaction ≤ 100ms.\n- QA: verify empty-state and no-match messaging; confirm date rendering and sorting use updated_at.\n\n3) Navigation, docs, deploy\n- Navigation: add “InformData Sources” under Data/Coverage in the site nav; slug /coverage/informdata-sources.\n- Docs: docs/coverage/informdata_sources.md (dataset fields, source of truth, update cadence, how to regenerate, known limitations).\n- CI/build: ensure data/coverage/informdata_natcrim.json is included in build artifacts; add predeploy check to run make natcrim-validate.\n- Commit/PR: include dataset, schema, parser, page, nav, and docs; obtain review from Sales Ops.\n- Deploy: merge to main and confirm Vercel production deploy; smoke test page load, filters, exports.\n\nAcceptance criteria\n- JSON dataset generated at data/coverage/informdata_natcrim.json, passes schema validation, and includes the six specified fields for all rows.\n- Interactive page renders the full dataset, supports the listed filters/search, and exports filtered results to CSV/JSON accurately.\n- Deep links reproduce the same filtered view when shared.\n- New nav entry is present and routes to the page in production.\n- Documentation explains regeneration and field definitions; links from docs to the live page.\n- Vercel production deployment is successful and verified by Sales in a quick UAT.\n</info added on 2025-11-03T00:59:50.539Z>\n<info added on 2025-11-03T01:11:28.008Z>\nProgress\n- Implemented scripts/parse_natcrim_sources.py and generated content/pricing/informdata_natcrim_sources.json (2415 records).\n- Built site/coverage/informdata_source_list.html with State and Record Type filters plus keyword search; CSV export enabled.\n- Wired the page into site navigation and updated docs/data inventory to reference the dataset.\n\nFollow-ups to align with plan and acceptance criteria\n- Relocate and canonicalize dataset at data/coverage/informdata_natcrim.json; confirm fields state, jurisdiction, source_name, record_type, coverage_notes, updated_at (UTC ISO 8601). Update the page’s data fetch to /data/coverage/informdata_natcrim.json.\n- Add schemas/coverage/informdata_natcrim.schema.json and a validate command in scripts/parse_natcrim_sources.py; add Makefile targets natcrim-parse and natcrim-validate; wire predeploy to run make natcrim-validate.\n- Align page artifacts to site/coverage/informdata_sources.html (+ .css/.js) and adjust imports/links; set slug to /coverage/informdata-sources and nav label “InformData Sources” under Data/Coverage.\n- Implement deep-linking by encoding current filters in URL query params.\n- Add JSON export alongside CSV and include applied filters in export filenames.\n- Complete table UX: sortable columns, sticky header, client-side pagination, and a row count badge; ensure free-text search covers coverage_notes.\n- Accessibility: keyboard navigation and ARIA labels; verify color contrast compliance.\n- Performance: meet ≤200ms initial render on 5k rows and ≤100ms filter interactions.\n- Tests: add tests/test_natcrim_parser.py with fixtures for merged cells, missing dates, multiline notes; verify dedupe and updated_at mapping.\n- Docs: create docs/coverage/informdata_sources.md with field definitions, source of truth, regeneration steps, update cadence, limitations; link to the live page.\n- Deploy: include dataset in build artifacts; open PR for schema/parser/page/nav/docs; verify Vercel production deploy, smoke test deep links/exports/date sorting, and obtain Sales UAT.\n</info added on 2025-11-03T01:11:28.008Z>\n<info added on 2025-11-03T01:13:21.443Z>\n- Verified repo contains commit \"feat: publish InformData source directory (task 10.3)\".\n- Verified Vercel production deployment is live at https://informdata-sales-sheet-5y3bvr933-vuplicity.vercel.app; /informdata_source_list.html loads and functions (State/Record Type filters, keyword search, CSV export).\n- Nav entry present; page reachable via site navigation and direct URL; no console errors observed in smoke test.\n</info added on 2025-11-03T01:13:21.443Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10,
            "updatedAt": "2025-11-03T01:11:39.723Z",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Document InformData webhook/SDK integration",
            "description": "Author developer-facing docs covering the TypeScript SDK (v0.9.2), async webhook sequencing, and ordering guidance using latest API material.",
            "details": "<info added on 2025-11-03T01:26:57.628Z>\nPlan and deliverables\n\nObjectives\n- Extract authoritative webhook behaviors and ordering guidance from the TypeScript SDK v0.9.2 and the latest InformData API docs.\n- Author a developer-facing Markdown guide that standardizes async orchestration, sequencing rules, and SDK usage patterns.\n- Publish the guide in-repo, link it from the sales collateral README, and trigger docs redeploy if required.\n\nWork steps\n1) Inspect SDK tarball and API docs\n- Acquire the TypeScript SDK v0.9.2 tarball (npm pack or internal registry) and extract it locally.\n- Review SDK README, type definitions, and examples to enumerate:\n  - Supported webhook event types and payload schemas.\n  - Signature verification scheme (headers, algorithm, clock skew tolerance).\n  - Retry semantics, delivery guarantees, and deduplication signals (event id, sequence, idempotency).\n  - Ordering/placement guidance and any sequencing metadata exposed by the SDK.\n- Cross-check against the latest InformData API docs; note any discrepancies and capture canonical field names and headers.\n- Record references (SDK version, doc URLs/commit hashes) in a sources section for traceability.\n\n2) Draft developer doc (Markdown)\n- File: docs/developers/informdata_webhooks_and_sdk.md\n- Required sections:\n  - Overview and versioning: scope, SDK v0.9.2, API docs snapshot date.\n  - Architecture: async webhooks, eventual consistency model, idempotency strategy.\n  - Event model: list of event categories, payload envelope, metadata fields used for ordering and dedupe.\n  - Signature verification: header names, algorithm, replay window, failure handling.\n  - Sequencing rules: handling out-of-order and duplicate deliveries; exactly-once processing goals; persistence of last-seen sequence; reconciliation job pattern.\n  - Ordering guidance: when to place combined vs. split orders; dependencies across sources; retries/backoff; cancellation/update flows.\n  - SDK usage patterns (TypeScript):\n    - Initializing the client securely.\n    - Placing an order with idempotency keys.\n    - Verifying webhook signatures (Express middleware example).\n    - Deduping and sequencing with a lightweight store (e.g., Redis/SQL).\n    - Reconciling state via polling endpoints as a safety net.\n  - Local development and testing: ngrok/tunneling, sample curl payloads, replaying events, fixture management.\n  - Observability and failures: logging, metrics, alerting, poison-queue pattern.\n  - Security and compliance: secret management, payload validation, PII handling.\n  - Appendix: glossary, example schemas, error codes, rate limits.\n- Include at least one end-to-end code path (place order → receive events → finalize) with TypeScript snippets aligned to v0.9.2 types.\n\n3) Publish and link\n- Commit docs/developers/informdata_webhooks_and_sdk.md.\n- Add a pointer from docs/sales-collateral/README.md under “Technical references” to the new guide.\n- If a docs site exists, update its navigation and run the docs build/deploy workflow.\n- Add minimal runnable example:\n  - examples/typescript/informdata-webhook-demo/ (Express webhook handler, signature verification, sequence/dedupe stub).\n  - .env.example with required vars (no secrets).\n\nAcceptance criteria\n- docs/developers/informdata_webhooks_and_sdk.md exists, references SDK v0.9.2 explicitly, and cites the API docs source.\n- The guide documents webhook verification, retry/dedupe, and sequencing rules without contradicting the latest API docs.\n- Includes TypeScript examples for client init, order placement with idempotency, webhook verification, and reconciliation.\n- A link to the guide is present in docs/sales-collateral/README.md.\n- Example app compiles and can receive and validate a sample webhook locally.\n- CI link checks and Markdown linting pass; docs build (if applicable) completes successfully.\n</info added on 2025-11-03T01:26:57.628Z>\n<info added on 2025-11-03T02:19:22.320Z>\nProgress update:\n- Extracted SDK/Webhook details from typescript-sdk-prod-v0.9.2.tar and InformData API request/response samples.\n- Drafted integrations/informdata_sdk_webhooks.md covering auth, event types, sequencing guidance, and error handling.\n- Added reference link from docs/notes/implementation_overview.md.\n\nFollow-ups to meet acceptance criteria:\n- Move/rename the guide to docs/developers/informdata_webhooks_and_sdk.md and add an Overview/versioning section plus a Sources section citing SDK v0.9.2 and the API docs snapshot date/URLs.\n- Expand required sections: architecture; signature verification (header names, algorithm, replay window, failure handling); ordering guidance (combined vs split orders, dependencies, retries/backoff, cancellation/update flows); idempotency/dedupe and sequencing rules; reconciliation job pattern.\n- Add TypeScript examples: client initialization, order placement with idempotency keys, Express middleware for webhook signature verification, dedupe/sequence store usage (Redis/SQL), reconciliation via polling, and a full end-to-end flow.\n- Create examples/typescript/informdata-webhook-demo/ with .env.example and a webhook replay script; ensure it compiles and validates a sample event locally.\n- Link the guide from docs/sales-collateral/README.md under “Technical references”; update docs site navigation and run docs build/deploy if applicable.\n- Run CI link checks and Markdown linting.\n\nNotes:\n- During expansion, log and reconcile any SDK/API discrepancies; standardize on canonical field names and headers.\n</info added on 2025-11-03T02:19:22.320Z>",
            "status": "in-progress",
            "dependencies": [],
            "parentTaskId": 10,
            "updatedAt": "2025-11-03T01:26:08.867Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-11-03T01:26:08.867Z"
      },
      {
        "id": "11",
        "title": "Establish shared schemas, directory scaffolding, and validation utilities for sales-pricing datasets",
        "description": "Create a reusable directory structure, JSON Schema/Pandera building blocks, and a CLI-based validator to support all sales-pricing datasets, aligning with the repo conventions and PRD. Provide templates and scaffolds that Tasks 3 and 5 will extend for dataset-specific schemas.",
        "details": "Scope and objectives\n- Provide a standardized foundation for all pricing-related datasets (costs, competitor MSRPs, derived tables) without implementing dataset-specific schemas that are in scope of Tasks 3 and 5.\n- Deliver a shared directory layout, common JSON Schema definitions, Pandera base column types, and a CLI validator to ensure consistent validation/error reporting across datasets.\n- Align naming, paths, and conventions with Task 1 and prd_sales_sheet.txt.\n\nRepository structure and scaffolding\n- Create directories with .gitkeep where appropriate:\n  - data/pricing/\n  - schemas/pricing/\n  - schemas/pricing/defs/\n  - schemas/pricing/templates/\n  - manifests/\n  - src/pricing_validation/\n  - scripts/\n  - tests/data/pricing/\n  - tests/pricing_validation/\n- Add docs/pricing/datasets.md documenting conventions: file naming, date fields (ISO-8601), currency formatting (ISO 4217), SKU normalization, provenance expectations, and validation workflows.\n\nShared JSON Schema assets (Draft 2020-12)\n- schemas/pricing/defs/common.json: shared $defs for:\n  - sku, vendor, product_name, region, currency (enum via ISO 4217), money, date, date_range, url, approval_status (enum: draft, proposed, approved, retired), evidence_ref.\n- schemas/pricing/_meta/dataset_manifest.schema.json: schema for a dataset manifest that describes dataset versioning, primary keys, schema path, and provenance fields.\n- schemas/pricing/templates/csv_schema.template.json: a template with placeholders (${dataset_name}, ${columns}, ${primary_key}) and guidance comments for authors of dataset-specific schemas.\n- schemas/README.md: explain modularization, $ref usage to defs/common.json, and how to extend for new datasets.\n\nPython validation utilities (shared, reusable)\n- src/pricing_validation/__init__.py\n- src/pricing_validation/cli.py (Typer-based):\n  - pricing-validate csv <csv_path> --schema <json_schema_path> [--dialect <excel|unix>] [--fail-level <error|warning>] [--format <text|json>]\n  - pricing-validate manifest <manifest_path> --schema schemas/pricing/_meta/dataset_manifest.schema.json\n  - pricing-scaffold schema --name <dataset_name> [--out schemas/pricing/] to generate a dataset schema from the template and create a stub CSV and README.\n- src/pricing_validation/jsonschema_validator.py:\n  - Load CSV via pandas with dtype hints.\n  - Validate row-wise objects against a JSON Schema using jsonschema; aggregate errors with row index, column, validator, message; exit non-zero on errors.\n- src/pricing_validation/pandera_base.py:\n  - Define Pandera base column types (e.g., SKUCol, CurrencyCol, MoneyCol, ISODateCol) and a BaseCSVValidator to apply DataFrame-level constraints (e.g., unique primary key, non-null required columns).\n- src/pricing_validation/reporting.py:\n  - Standardize error reporting structure and JSON output: {file, row, column, rule, message, context}.\n\nDeveloper experience and automation\n- requirements.txt: pandas, pandera, jsonschema, typer, tabulate, python-dateutil, ruamel.yaml.\n- Makefile targets:\n  - make scaffold_pricing: create directories and .gitkeep files.\n  - make validate_pricing FILE=path.csv SCHEMA=path.json: invoke CLI.\n- scripts/scaffold_pricing_repo.py: idempotent bootstrap to create directories, seed templates, and write example manifests.\n- .pre-commit-config.yaml: add a local hook to run pricing-validate on changed CSVs under data/pricing/ against a declared schema if a matching manifest exists.\n- .github/workflows/validate-pricing.yml (CI): on PRs, run pricing-validate for any changed CSVs and manifest validation.\n\nExample artifacts and fixtures\n- manifests/pricing_datasets.yaml: list of datasets with schema paths, primary keys, and data file paths (used by CI and pre-commit).\n- tests/data/pricing/example_ok.csv and example_bad.csv to exercise validators.\n- tests/pricing_validation/test_cli.py: CLI behavior, exit codes, JSON output shape.\n- tests/pricing_validation/test_pandera_base.py: primary key uniqueness, required columns, currency/ISO date checks.\n\nCollaboration with Tasks 3 and 5\n- Do not implement the final schemas for InformData costs or competitor MSRPs in this task.\n- Provide templates and shared definitions that Tasks 3 and 5 consume to author:\n  - schemas/pricing/informdata_costs.schema.json (Task 3)\n  - schemas/pricing/competitor_msrps.schema.json (Task 5)\n- Ensure the CLI supports these future schemas without changes by adhering to shared conventions.\n",
        "testStrategy": "Prerequisites\n- Ensure Task 1 conventions are available (repo structure and PRD). Create and activate a virtualenv.\n- pip install -r requirements.txt\n\nScaffolding verification\n1) Run: python scripts/scaffold_pricing_repo.py\n   - Confirm directories exist: data/pricing/, schemas/pricing/, schemas/pricing/defs/, schemas/pricing/templates/, manifests/, src/pricing_validation/, tests/.\n   - Confirm files exist with content: schemas/pricing/defs/common.json, schemas/pricing/_meta/dataset_manifest.schema.json, schemas/pricing/templates/csv_schema.template.json, docs/pricing/datasets.md.\n\nCLI availability and help\n2) Run: python -m pricing_validation.cli --help\n   - Verify commands: csv, manifest, and scaffold schema are listed.\n\nManifest validation\n3) Open manifests/pricing_datasets.yaml and ensure it conforms to the documented fields (dataset name, schema path, data path, primary key, approvals).\n4) Run: pricing-validate manifest manifests/pricing_datasets.yaml --schema schemas/pricing/_meta/dataset_manifest.schema.json\n   - Expect exit code 0 and a success message.\n\nCSV validation (positive)\n5) Place tests/data/pricing/example_ok.csv and a corresponding tests/data/pricing/example_ok.schema.json created from the template referencing defs/common.json types.\n6) Run: pricing-validate csv tests/data/pricing/example_ok.csv --schema tests/data/pricing/example_ok.schema.json --format json\n   - Expect exit code 0 and empty errors array (or a success summary).\n\nCSV validation (negative)\n7) Run: pricing-validate csv tests/data/pricing/example_bad.csv --schema tests/data/pricing/example_ok.schema.json --format json\n   - Expect non-zero exit code.\n   - JSON output should include: file, row, column, rule, message, and a total error count > 0.\n\nPandera base checks\n8) Add a Pandera-based schema using src/pricing_validation/pandera_base.py in a small test to enforce primary key uniqueness and ISO date format.\n   - Run pytest -q and confirm tests pass.\n\nPre-commit hook (local)\n9) Install pre-commit and run: pre-commit run -a\n   - Verify the pricing-validate hook runs on CSVs under data/pricing/ and surfaces any schema violations.\n\nCI config presence\n10) Confirm .github/workflows/validate-pricing.yml exists and references the manifests file to validate changed CSVs on pull requests (lint-only; no network calls).\n\nDocumentation\n11) Open docs/pricing/datasets.md and verify it explains:\n   - Directory layout, schema conventions, how to create a new dataset schema from the template, and how to use the CLI and Make targets.\n",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Plan Task 11 execution",
            "description": "Outline schema files, directory setup, and validation tool approach before implementation.",
            "details": "<info added on 2025-10-30T11:33:49.840Z>\nPlan for Subtask 11.1\n\n- PRD review\n  - Read ./prd_sales_sheet.txt and extract shared field requirements (names, types, required/nullable, enums, date/currency formats, uniqueness, key constraints) applicable across pricing datasets.\n  - Produce a concise summary of shared requirements in docs/data_schemas/shared_requirements.md and map them to base column types used in schemas.\n\n- Source data inventory (current and planned)\n  - Current: data/pricing/informdata_costs.csv (from Tasks 2/3).\n  - Planned/upstream: competitor MSRP inputs (Task 6, path under data/pricing/ to be finalized) and pricing computation outputs (Task 4, derived tables under data/pricing/derived/).\n  - Capture the inventory with owners, status, and expected availability in docs/data_schemas/data_inventory.md.\n\n- Target directories\n  - data/pricing/ (raw and derived CSV/JSON; include .gitkeep and samples/ for validator checks).\n  - docs/data_schemas/ (schema docs, shared requirements, inventory, and index).\n  - docs/data_schemas/schemas/ (YAML schema files).\n  - scripts/validation/ (CLI validator and helpers).\n\n- Schema format decision\n  - Use YAML for canonical schemas plus companion Markdown documentation.\n  - YAML keys: dataset_id, version, description, file_pattern, primary_key (list), required_files (optional), fields[].name, dtype, required, allow_null, enum, regex, min, max, unique, example, notes; dataset_constraints (cross-field rules).\n  - Author per-dataset YAMLs: docs/data_schemas/schemas/base_costs.schema.yaml, competitor_msrps.schema.yaml, pricing_outputs.schema.yaml.\n  - Provide a short explainer in docs/data_schemas/schema_conventions.md (naming, dtypes, constraints, extensibility to Pandera/JSON Schema).\n\n- Validator responsibilities (scripts/validation/validate_pricing_data.py)\n  - Load YAML schema, match input via file_pattern or explicit --schema path.\n  - Enforce schema: column presence/order (if specified), dtypes, required/nullability, enums/regex, uniqueness (single/composite), primary_key integrity, numeric/date bounds, and cross-field rules (e.g., price >= cost).\n  - Sample data check: verify presence of sample files under data/pricing/samples/ and ensure they pass validation with minimum row thresholds; run suitable for CI.\n  - Outputs: human-readable summary to stdout and machine-readable JSON report (--report-json path).\n  - CLI flags: --input <file/dir>, --schema <yaml>, --fail-fast, --strict, --report-json <path>, --dataset-id <name>, --sample-check.\n  - Exit codes: 0 on success, non-zero on any schema or sample check failure.\n\n- Acceptance criteria reference paths\n  - PRD: ./prd_sales_sheet.txt (authoritative requirements).\n  - Task 1 conventions: docs/conventions/ (path from Task 1; ensure alignment of naming and paths).\n  - This task acceptance mapping: docs/data_schemas/acceptance/task11.md (links to schemas, validator CLI usage, and CI sample-check).\n  - Validator usage doc: docs/data_schemas/validator.md (examples and expected outputs).\n</info added on 2025-10-30T11:33:49.840Z>\n<info added on 2025-10-30T11:40:42.392Z>\nProgress update:\n- Directory scaffolding created: data/pricing/, data/pricing/samples/, docs/data_schemas/, docs/data_schemas/schemas/, scripts/validation/.\n- Documentation authored: docs/data_schemas/shared_requirements.md, docs/data_schemas/data_inventory.md, docs/data_schemas/schema_conventions.md, docs/data_schemas/validator.md, docs/data_schemas/acceptance/task11.md.\n- YAML schemas added: docs/data_schemas/schemas/base_costs.schema.yaml, docs/data_schemas/schemas/competitor_msrps.schema.yaml, docs/data_schemas/schemas/pricing_outputs.schema.yaml.\n- Sample CSVs added under data/pricing/samples/ for each dataset.\n- Python validator implemented at scripts/validation/validate_pricing_data.py and executed against all sample datasets, producing JSON validation reports and referenced usage in the docs.\n</info added on 2025-10-30T11:40:42.392Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 11,
            "updatedAt": "2025-10-30T11:40:51.405Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-10-30T11:40:51.405Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-04T18:45:20.255Z",
      "taskCount": 11,
      "completedCount": 9,
      "tags": [
        "master"
      ]
    }
  }
}