# Task ID: 10
# Title: Generate collateral-ready pricing tables from approved InformData costs and margin policy
# Status: in-progress
# Dependencies: 3, 4
# Priority: high
# Description: Build a tool that filters approved InformData costs, applies the margin policy via the pricing engine, and emits collateral-ready pricing tables in CSV/JSON/YAML for downstream Markdown/PDF generation.
# Details:
Scope and objectives
- Read validated InformData cost data and select only approved, in-force rows as of a given date.
- Apply target margins using the pricing computation engine (Task 4) to produce SKU-level list prices and related derived fields suitable for sales collateral.
- Emit machine-readable and human-consumable tables plus a manifest for provenance and reproducibility.

Inputs and assumptions
- Costs input: data/pricing/informdata_costs.csv (validated by Task 3). The schema must include columns such as: sku, sku_name, description, unit, cost_basis, approval_status, effective_date, end_date (nullable), currency, category, notes. approval_status should include values like approved, draft, deprecated.
- Pricing policy/config: configs/pricing/policy.yaml (margin targets, rounding rules, currency handling) and optional configs/pricing/rounding.yaml.
- Pricing engine: import and use the public API produced in Task 4 (e.g., pricing.engine.compute(df_costs, policy) or equivalent CLI/module), not re-implementing pricing logic.

Artifacts to add
- Module: src/pricing/collateral_tables.py (functions to load, filter, compute, and export tables).
- CLI: scripts/build_collateral_pricing_tables.py with options: --as-of-date YYYY-MM-DD, --policy PATH, --output-dir PATH, --region, --channel, --currency, --include-skus, --exclude-skus, --rounding-profile, --strict.
- Output tables (created on run):
  - derived/pricing/collateral/informdata_pricing_tables.csv
  - derived/pricing/collateral/informdata_pricing_tables.json
  - derived/pricing/collateral/informdata_pricing_tables.yaml (Jinja-friendly context for collateral rendering)
  - derived/pricing/collateral/manifest.json (inputs, checksums, git SHA, as_of_date, policy version, engine version)
- Output schema: schemas/pricing/collateral_pricing_tables.schema.json defining required columns and constraints.

Output columns (minimum)
- sku, sku_name, category, unit
- cost_basis (numeric), target_margin_pct (numeric)
- computed_price (numeric, pre-round), list_price (numeric, post-round), currency
- effective_date, end_date (nullable)
- channel, region (nullable), notes (nullable)
- source_cost_version (e.g., hash or version string), policy_version, engine_version

Filtering and computation rules
- Only include rows where approval_status == "approved" and effective_date <= as_of_date and (end_date is null or end_date >= as_of_date).
- If multiple approved rows exist for the same SKU, choose the one with the most recent effective_date not after as_of_date; error if overlapping effective windows unless --allow-overlap is set (optional future flag).
- Pass normalized costs to Task 4 pricing engine along with policy for margin and any AI/SaaS overheads defined therein. Do not duplicate pricing logic here; this task orchestrates and shapes collateral-ready outputs.
- Apply rounding rules from policy/rounding profiles (e.g., 0.99 endings, currency minor units). Validate rounded values remain >= computed_price and respect minimums.
- Support optional filters: by region/channel and SKU allow/deny lists.

Implementation notes
- Use pandas and pandera for IO/validation; typer for CLI; jsonschema for output schema; python-dateutil for dates.
- Implement a small adapter to call the Task 4 engine. Example shape:
  from pricing.engine import compute_prices
  df_prices = compute_prices(df_costs=approved_df, policy=policy_cfg)
- Build a deterministic export: stable column order, sorted by category then sku.
- Write a manifest with SHA256 checksums of inputs (costs CSV, policy files), output files, git commit, timestamp, and as_of_date.
- Document usage in README.md section: how to run the CLI and where outputs land.

Error handling and logging
- Fail fast if input schema invalid (reuse Task 3 validator if available).
- Emit clear errors for missing policy keys, unknown currencies, or overlapping cost windows.
- Log summary: #SKUs in, #SKUs approved, #SKUs priced, #errors, min/median/max margins.

Acceptance criteria
- Running the CLI with a valid as_of_date produces all outputs under derived/pricing/collateral/ matching the output schema and including only approved SKUs.
- Prices are computed by Task 4 engine and match the engine’s outputs (modulo rounding), with policy/rounding versions stamped in manifest.json.
- YAML output is loadable as a Jinja context for downstream collateral rendering (Task 7 will consume it).

# Test Strategy:
Prereqs
- Ensure Task 3 and Task 4 are complete and available in the environment.
- Create and activate a virtualenv; pip install -r requirements.txt.

Data and config setup
1) Validate costs input
- Run the Task 3 validator against data/pricing/informdata_costs.csv and confirm success.

2) Prepare policy
- Confirm configs/pricing/policy.yaml exists with target margins and rounding profile; adjust as needed for the test.

Functional test
3) Run the CLI
- python scripts/build_collateral_pricing_tables.py --as-of-date 2025-01-01 --policy configs/pricing/policy.yaml --output-dir derived/pricing/collateral
- Expect exit code 0.

4) File existence and structure
- Confirm the following exist:
  - derived/pricing/collateral/informdata_pricing_tables.csv/json/yaml
  - derived/pricing/collateral/manifest.json
  - schemas/pricing/collateral_pricing_tables.schema.json

5) Schema validation
- Use a jsonschema/pandera check to validate informdata_pricing_tables.json/csv against schemas/pricing/collateral_pricing_tables.schema.json.

6) Content checks
- Confirm only rows with approval_status == approved (from input) are present and effective_date windows include the as_of_date.
- Spot-check a few SKUs: recompute through the Task 4 engine or compare against engine outputs to ensure computed_price matches pre-round values and list_price reflects rounding rules.
- Verify currency and rounding to minor units are correct; list_price >= computed_price.

7) Determinism
- Re-run the CLI with identical inputs; diff outputs should show no changes.

8) Manifest integrity
- Open derived/pricing/collateral/manifest.json and confirm it contains: input file paths and SHA256 checksums, git SHA, as_of_date, engine version, policy version, and checksums for output files.

9) Error-path tests
- Change a row’s approval_status to draft and re-run; confirm the SKU is excluded.
- Create an overlapping effective window for a SKU and re-run with --strict (default); expect a non-zero exit and a clear error message.

10) Jinja context sanity
- In a Python REPL: load the YAML output and access a few fields (e.g., sku, list_price) to ensure it’s a clean mapping suitable for templating.

# Subtasks:
## 1. Align collateral branding with Vuplicity [done]
### Dependencies: None
### Description: Replace residual Booplicity mentions in generated pricing outputs and collateral, regenerate tables, and ensure UI copy clearly distinguishes InformData vendor costs from Vuplicity automation/platform allocations.
### Details:
<info added on 2025-10-31T01:47:48.976Z>
Plan and execution details

1) Audit for legacy names and guidance
- Scan code, templates, docs, and generated assets for residual Booplicity/Gamma references:
  - rg -n "Booplicity|booplicity" --hidden --glob '!venv/**' --glob '!build/**' .
  - rg -n "Gamma|gamma\.?com" --hidden --glob '!venv/**' --glob '!build/**' .
- Expected allowed Gamma references: only under task_9 packaging code and docs (see Acceptance Criteria). All others must be removed or updated.
- Update brand constants/placeholders:
  - templates, Jinja vars, and context builders should use brand_name = "Vuplicity".
  - Replace logo paths to assets/branding/vuplicity/* and remove assets/branding/booplicity/* if unused.
- Touchpoints to review/update:
  - templates/collateral/*.j2, templates/email/*.j2
  - tools/pricing/*.py, tools/collateral/*.py
  - docs/collateral/*, docs/deploy/*, README.md
  - .taskmaster/tasks/*.json and task descriptions
  - build scripts and Makefile/Taskfile if present

2) Regenerate pricing CSV/HTML with Vuplicity messaging and clarified copy
- Ensure inputs and engine are valid:
  - python tools/pricing/validate_costs.py --input data/pricing/informdata_costs.csv
  - python tools/pricing/compute_prices.py --costs data/pricing/informdata_costs.csv --as-of today --out build/pricing/vuplicity/
- Emit CSV/JSON/YAML plus manifest:
  - python tools/pricing/build_pricing_tables.py --in build/pricing/vuplicity/derived.json --formats csv json yaml --out build/collateral/tables/
  - Confirm manifest at build/collateral/tables/manifest.json (includes source commit, schema versions, as_of date).
- Render HTML collateral with updated copy:
  - Update templates/collateral/pricing_vuplicity.html.j2 to include the following content model:
    - Headline: “Vuplicity pricing for InformData-sourced searches”
    - Columns: “InformData vendor cost (pass-through)”, “Vuplicity automation/platform add-ons”, “Total MSRP”
    - Footnotes:
      1) “InformData vendor cost is a direct pass-through and may change per vendor notice.”
      2) “Vuplicity automation/platform add-ons cover orchestration, retries, monitoring, reporting, and SLA tooling; they are configurable.”
      3) “Total MSRP = vendor cost + selected add-ons; volume tiers and contractual discounts may apply.”
    - Badge/tagline: “Powered by Vuplicity automation and platform controls”
  - Render:
    - python tools/collateral/render_pricing_html.py --tables build/collateral/tables/ --template templates/collateral/pricing_vuplicity.html.j2 --out build/collateral/pricing_vuplicity.html

3) Update docs and deployment notes; rerun builders to keep outputs in sync
- Remove or rewrite stale Gamma guidance in:
  - docs/collateral/branding.md (replace Booplicity examples with Vuplicity)
  - docs/deploy/pricing_builders.md (current build steps; no Gamma-specific content unless referencing Task 9)
  - docs/gamma.md (either delete if obsolete for this flow or replace with a pointer: “Gamma packaging is handled by Task 9”)
- Restate deployment notes:
  - Environment: python -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt
  - Commands: run validate_costs, compute_prices, build_pricing_tables, render_pricing_html as above
  - Outputs: build/collateral/tables/* and build/collateral/pricing_vuplicity.html are the authoritative artifacts for Sales
- Rerun the full builder sequence and commit updated outputs with a single version bump in build/collateral/tables/manifest.json.

Acceptance criteria (DoD)
- Zero occurrences of “Booplicity” in repo and generated outputs:
  - rg -n "Booplicity|booplicity" --hidden --glob '!venv/**' . returns no matches.
- “Gamma” appears only in Task 9 code/docs:
  - Allowed: task_9 source files under tools/packaging/gamma/* and docs/packaging/gamma.md (or equivalent), plus Task 9 manifest templates.
  - All other Gamma references removed or replaced with Task 9 pointer.
- Updated HTML collateral present at build/collateral/pricing_vuplicity.html with:
  - Correct headline, columns, and the three footnotes above.
- Tables emitted at build/collateral/tables/ in CSV, JSON, and YAML, with a manifest containing source commit, as_of date, and schema versions.
- Docs updated: branding, deployment notes, and Gamma guidance reconciled; links resolve; examples show Vuplicity only.
- Pricing recomputed from approved InformData costs (validator passes) and matches the engine outputs for the same as_of date.

Notes for reviewers
- Verify that “InformData vendor cost (pass-through)” is labeled exactly and appears in column headers and legend.
- Confirm add-on descriptions enumerate orchestration, retries, monitoring, reporting, and SLA tooling.
- Confirm logo and brand colors are from assets/branding/vuplicity/ and no Booplicity assets remain.
</info added on 2025-10-31T01:47:48.976Z>
<info added on 2025-10-31T03:09:22.170Z>
Progress update

- Regenerated internal pricing and consolidated tables with Vuplicity automation/platform assumptions; emitted CSV/JSON/YAML plus updated manifest under build/collateral/tables/ (as_of and source commit populated).
- Published matching JSON used by HTML dashboards and wired templates to consume it during render_pricing_html.
- Swapped branding and UI copy across pricing breakout, county fee, and national scan pages:
  - Column headers: “InformData vendor cost (pass-through)”, “Vuplicity automation/platform add-ons”, “Total MSRP”.
  - Footnotes (1–3) and “Powered by Vuplicity automation and platform controls” badge included.
  - Logos/colors now sourced from assets/branding/vuplicity/.
- Scrubbed Gamma references from docs and PRD; retained only Task 9 packaging pointers. Updated collateral instructions to reflect current builder workflow.
- Verification:
  - rg for “Booplicity|booplicity” returns no matches.
  - rg for “Gamma|gamma\.?com” limited to allowed Task 9 packaging paths.
  - HTML renders and dashboards load the new JSON; callouts explicitly distinguish InformData vs. Vuplicity costs.
- Next: finalize commit with single version bump in build/collateral/tables/manifest.json and request reviewer sign-off.
</info added on 2025-10-31T03:09:22.170Z>

## 2. Publish collateral updates and new Vercel pages [done]
### Dependencies: None
### Description: Deploy refreshed pricing breakout plus dedicated court fee and database coverage pages, ensuring Vercel site renders each asset with Vuplicity messaging.
### Details:
<info added on 2025-11-02T23:14:21.914Z>
Implementation plan and deliverables

1) County fee page
- Source data: copy latest county fee output into public/data/county_fees.json (fallback: convert CSV to JSON with scripts/tools/csv_to_json.py).
- Page: pages/court-fees.html (static) with client-side filtering (no server code). Include:
  - Filters: State (multi-select), County (typeahead), Court type (if present in data), Fee range, “Only show surcharges/notes” toggle.
  - Results table with sticky header, virtualized rendering for large lists, CSV export of filtered view.
  - Provenance banner and Vercel deployment guidance section (explain client-side JSON fetch, no serverless required, cache headers).
  - Vuplicity messaging block and brand styles.
- JS module: public/js/court_fees.js to handle fetch(/data/county_fees.json), filters, URL state (query params), and accessibility.
- Build hook: add script to sync latest data to public/data on build (e.g., npm run sync-data:fees).

2) Databases coverage page
- Source: docs/downloads.md (and linked downloads) → normalize into public/data/database_coverage.json via scripts/coverage/extract_downloads.py.
- Page: pages/database-coverage.html with:
  - Summary cards (products covered, jurisdictions count, last refresh date).
  - Coverage table: product, jurisdiction scope, coverage %, last updated, source URL, known limitations, provenance id.
  - “How to interpret coverage” section for reps, with explicit data provenance, refresh cadence, and caveats.
  - Link to data dictionary and download of the JSON/CSV snapshot.
  - Vuplicity messaging block.
- JS module: public/js/database_coverage.js for client-side rendering, filtering, sorting.

3) Wire into site and deploy
- Navigation: update partials/header.html and partials/footer.html to add “Court Fees” and “Database Coverage” links; ensure existing Pricing Breakout link remains visible.
- Pricing breakout: verify pages/pricing-breakout.html reads from public/data/pricing_breakout.json (refresh data during build).
- SEO: update public/sitemap.xml and add noindex to any internal-only staging variants if applicable.
- Local sanity check:
  - vercel pull
  - vercel dev (or npm run dev) and open /court-fees and /database-coverage
  - Validate filters, export buttons, mobile layout, and that JSON loads with 200.
- Build and deploy:
  - npm run build (ensures data sync tasks run)
  - vercel build
  - vercel deploy --prebuilt --prod
- Post-deploy validation: open production URLs, confirm cache-control headers on JSON, check console for errors, run basic Lighthouse pass.

Acceptance criteria
- Court Fees page loads latest data from public/data/county_fees.json; filters work, export functions, and guidance/provenance text is present; styling matches Vuplicity.
- Database Coverage page renders from public/data/database_coverage.json; includes provenance, interpretation guidance, and working filters/sorts.
- Header/footer include links to Court Fees, Database Coverage, and Pricing Breakout; sitemap includes both new pages.
- Local dev and production builds succeed; site deployed via Vercel CLI with no console errors; JSON assets are cacheable but bust on redeploy.
- All content is client-rendered (no serverless functions), accessible (keyboard/tab, aria labels), and mobile-responsive.
</info added on 2025-11-02T23:14:21.914Z>
<info added on 2025-11-02T23:21:18.146Z>
Progress
- Copied the standalone Court Fee dashboard and National Scan components into the repository root for Vercel static hosting.
- Added databases_coverage.html at the root; page renders from content/pricing/national_scan_sources.json (generated from the InformData workbook).
- Refreshed navigation across all pages to include the new collateral links and preserve Pricing Breakout.
- Committed (feat: expand collateral pages (task 10)) and pushed to GitHub.
- Deployed to Vercel production (deploy id: 6gczsKVuuVzn7ZdrftTMhv7W39Cq).

Follow-ups to align with plan and acceptance criteria
- Rename databases_coverage.html to database-coverage.html (hyphen) and add to public/sitemap.xml.
- Normalize data path: emit public/data/database_coverage.json (via a transform script) or update the JS fetch to the new source; include downloadable JSON/CSV snapshot and provenance/refresh cadence on the page.
- Verify Court Fees page sources public/data/county_fees.json; confirm filters, CSV export, accessibility, and Vuplicity messaging are present.
- Ensure Database Coverage page includes interpretation guidance, provenance IDs, known limitations, data dictionary link, and working filters/sorts.
- Confirm header/footer retain the Pricing Breakout link and that pages/pricing-breakout.html reads from public/data/pricing_breakout.json.
- Set cache-control headers on JSON and confirm cache bust on redeploy; run Lighthouse and console checks on production.
- Add/verify build sync tasks (e.g., npm run sync-data:fees and coverage data sync) and ensure client-only rendering (no serverless).
</info added on 2025-11-02T23:21:18.146Z>

## 3. Expose full InformData source list [done]
### Dependencies: None
### Description: Transform the InformData National Criminal workbook into a detailed dataset and publish an interactive page for Sales to browse coverage by state, jurisdiction, and record type.
### Details:
<info added on 2025-11-03T00:59:50.539Z>
Scope and deliverables
- Normalize the InformData National Criminal workbook into a JSON dataset with fields: state, jurisdiction, source_name, record_type, coverage_notes, updated_at (ISO 8601).
- Build an interactive HTML page for browsing/filtering/searching/exporting the dataset.
- Wire the page into site navigation, update docs, commit, and redeploy to Vercel.

Implementation plan
1) ETL and validation
- Input: data/sources/informdata_natcrim.xlsx
- Script: scripts/parse_natcrim_sources.py (Typer CLI: parse, validate)
- Output: data/coverage/informdata_natcrim.json
- Schema: schemas/coverage/informdata_natcrim.schema.json (types, required fields, enums where applicable)
- Parsing rules:
  - Trim whitespace; normalize state to USPS codes; unify jurisdiction casing; coalesce merged cells; preserve multiline coverage_notes.
  - Map workbook date fields to updated_at in UTC ISO 8601 (YYYY-MM-DD).
  - Drop empty rows; dedupe exact duplicates (state+jurisdiction+source_name+record_type).
- Makefile targets:
  - make natcrim-parse (generate JSON)
  - make natcrim-validate (run schema validation)
- Tests: add tests/test_natcrim_parser.py with sample fixtures covering edge cases (merged cells, missing dates, multiline notes).

2) Interactive page
- Files: site/coverage/informdata_sources.html, site/coverage/informdata_sources.css, site/coverage/informdata_sources.js
- Data load: fetch /data/coverage/informdata_natcrim.json (static asset served on Vercel)
- UX requirements:
  - Filters: multi-select State; text search on jurisdiction and source_name; checkboxes for record_type; free-text search across coverage_notes.
  - Results table: sortable columns, sticky header, client-side pagination; row count badge.
  - Export: buttons to download current filtered view as CSV and JSON; include applied filters in export filename.
  - Shareable links: encode current filters in URL query params for deep-linking.
  - Accessibility: keyboard navigation, ARIA labels, color-contrast compliant.
  - Performance: initial render ≤ 200ms on 5k rows; filter interaction ≤ 100ms.
- QA: verify empty-state and no-match messaging; confirm date rendering and sorting use updated_at.

3) Navigation, docs, deploy
- Navigation: add “InformData Sources” under Data/Coverage in the site nav; slug /coverage/informdata-sources.
- Docs: docs/coverage/informdata_sources.md (dataset fields, source of truth, update cadence, how to regenerate, known limitations).
- CI/build: ensure data/coverage/informdata_natcrim.json is included in build artifacts; add predeploy check to run make natcrim-validate.
- Commit/PR: include dataset, schema, parser, page, nav, and docs; obtain review from Sales Ops.
- Deploy: merge to main and confirm Vercel production deploy; smoke test page load, filters, exports.

Acceptance criteria
- JSON dataset generated at data/coverage/informdata_natcrim.json, passes schema validation, and includes the six specified fields for all rows.
- Interactive page renders the full dataset, supports the listed filters/search, and exports filtered results to CSV/JSON accurately.
- Deep links reproduce the same filtered view when shared.
- New nav entry is present and routes to the page in production.
- Documentation explains regeneration and field definitions; links from docs to the live page.
- Vercel production deployment is successful and verified by Sales in a quick UAT.
</info added on 2025-11-03T00:59:50.539Z>
<info added on 2025-11-03T01:11:28.008Z>
Progress
- Implemented scripts/parse_natcrim_sources.py and generated content/pricing/informdata_natcrim_sources.json (2415 records).
- Built site/coverage/informdata_source_list.html with State and Record Type filters plus keyword search; CSV export enabled.
- Wired the page into site navigation and updated docs/data inventory to reference the dataset.

Follow-ups to align with plan and acceptance criteria
- Relocate and canonicalize dataset at data/coverage/informdata_natcrim.json; confirm fields state, jurisdiction, source_name, record_type, coverage_notes, updated_at (UTC ISO 8601). Update the page’s data fetch to /data/coverage/informdata_natcrim.json.
- Add schemas/coverage/informdata_natcrim.schema.json and a validate command in scripts/parse_natcrim_sources.py; add Makefile targets natcrim-parse and natcrim-validate; wire predeploy to run make natcrim-validate.
- Align page artifacts to site/coverage/informdata_sources.html (+ .css/.js) and adjust imports/links; set slug to /coverage/informdata-sources and nav label “InformData Sources” under Data/Coverage.
- Implement deep-linking by encoding current filters in URL query params.
- Add JSON export alongside CSV and include applied filters in export filenames.
- Complete table UX: sortable columns, sticky header, client-side pagination, and a row count badge; ensure free-text search covers coverage_notes.
- Accessibility: keyboard navigation and ARIA labels; verify color contrast compliance.
- Performance: meet ≤200ms initial render on 5k rows and ≤100ms filter interactions.
- Tests: add tests/test_natcrim_parser.py with fixtures for merged cells, missing dates, multiline notes; verify dedupe and updated_at mapping.
- Docs: create docs/coverage/informdata_sources.md with field definitions, source of truth, regeneration steps, update cadence, limitations; link to the live page.
- Deploy: include dataset in build artifacts; open PR for schema/parser/page/nav/docs; verify Vercel production deploy, smoke test deep links/exports/date sorting, and obtain Sales UAT.
</info added on 2025-11-03T01:11:28.008Z>
<info added on 2025-11-03T01:13:21.443Z>
- Verified repo contains commit "feat: publish InformData source directory (task 10.3)".
- Verified Vercel production deployment is live at https://informdata-sales-sheet-5y3bvr933-vuplicity.vercel.app; /informdata_source_list.html loads and functions (State/Record Type filters, keyword search, CSV export).
- Nav entry present; page reachable via site navigation and direct URL; no console errors observed in smoke test.
</info added on 2025-11-03T01:13:21.443Z>

## 4. Document InformData webhook/SDK integration [in-progress]
### Dependencies: None
### Description: Author developer-facing docs covering the TypeScript SDK (v0.9.2), async webhook sequencing, and ordering guidance using latest API material.
### Details:
<info added on 2025-11-03T01:26:57.628Z>
Plan and deliverables

Objectives
- Extract authoritative webhook behaviors and ordering guidance from the TypeScript SDK v0.9.2 and the latest InformData API docs.
- Author a developer-facing Markdown guide that standardizes async orchestration, sequencing rules, and SDK usage patterns.
- Publish the guide in-repo, link it from the sales collateral README, and trigger docs redeploy if required.

Work steps
1) Inspect SDK tarball and API docs
- Acquire the TypeScript SDK v0.9.2 tarball (npm pack or internal registry) and extract it locally.
- Review SDK README, type definitions, and examples to enumerate:
  - Supported webhook event types and payload schemas.
  - Signature verification scheme (headers, algorithm, clock skew tolerance).
  - Retry semantics, delivery guarantees, and deduplication signals (event id, sequence, idempotency).
  - Ordering/placement guidance and any sequencing metadata exposed by the SDK.
- Cross-check against the latest InformData API docs; note any discrepancies and capture canonical field names and headers.
- Record references (SDK version, doc URLs/commit hashes) in a sources section for traceability.

2) Draft developer doc (Markdown)
- File: docs/developers/informdata_webhooks_and_sdk.md
- Required sections:
  - Overview and versioning: scope, SDK v0.9.2, API docs snapshot date.
  - Architecture: async webhooks, eventual consistency model, idempotency strategy.
  - Event model: list of event categories, payload envelope, metadata fields used for ordering and dedupe.
  - Signature verification: header names, algorithm, replay window, failure handling.
  - Sequencing rules: handling out-of-order and duplicate deliveries; exactly-once processing goals; persistence of last-seen sequence; reconciliation job pattern.
  - Ordering guidance: when to place combined vs. split orders; dependencies across sources; retries/backoff; cancellation/update flows.
  - SDK usage patterns (TypeScript):
    - Initializing the client securely.
    - Placing an order with idempotency keys.
    - Verifying webhook signatures (Express middleware example).
    - Deduping and sequencing with a lightweight store (e.g., Redis/SQL).
    - Reconciling state via polling endpoints as a safety net.
  - Local development and testing: ngrok/tunneling, sample curl payloads, replaying events, fixture management.
  - Observability and failures: logging, metrics, alerting, poison-queue pattern.
  - Security and compliance: secret management, payload validation, PII handling.
  - Appendix: glossary, example schemas, error codes, rate limits.
- Include at least one end-to-end code path (place order → receive events → finalize) with TypeScript snippets aligned to v0.9.2 types.

3) Publish and link
- Commit docs/developers/informdata_webhooks_and_sdk.md.
- Add a pointer from docs/sales-collateral/README.md under “Technical references” to the new guide.
- If a docs site exists, update its navigation and run the docs build/deploy workflow.
- Add minimal runnable example:
  - examples/typescript/informdata-webhook-demo/ (Express webhook handler, signature verification, sequence/dedupe stub).
  - .env.example with required vars (no secrets).

Acceptance criteria
- docs/developers/informdata_webhooks_and_sdk.md exists, references SDK v0.9.2 explicitly, and cites the API docs source.
- The guide documents webhook verification, retry/dedupe, and sequencing rules without contradicting the latest API docs.
- Includes TypeScript examples for client init, order placement with idempotency, webhook verification, and reconciliation.
- A link to the guide is present in docs/sales-collateral/README.md.
- Example app compiles and can receive and validate a sample webhook locally.
- CI link checks and Markdown linting pass; docs build (if applicable) completes successfully.
</info added on 2025-11-03T01:26:57.628Z>
<info added on 2025-11-03T02:19:22.320Z>
Progress update:
- Extracted SDK/Webhook details from typescript-sdk-prod-v0.9.2.tar and InformData API request/response samples.
- Drafted integrations/informdata_sdk_webhooks.md covering auth, event types, sequencing guidance, and error handling.
- Added reference link from docs/notes/implementation_overview.md.

Follow-ups to meet acceptance criteria:
- Move/rename the guide to docs/developers/informdata_webhooks_and_sdk.md and add an Overview/versioning section plus a Sources section citing SDK v0.9.2 and the API docs snapshot date/URLs.
- Expand required sections: architecture; signature verification (header names, algorithm, replay window, failure handling); ordering guidance (combined vs split orders, dependencies, retries/backoff, cancellation/update flows); idempotency/dedupe and sequencing rules; reconciliation job pattern.
- Add TypeScript examples: client initialization, order placement with idempotency keys, Express middleware for webhook signature verification, dedupe/sequence store usage (Redis/SQL), reconciliation via polling, and a full end-to-end flow.
- Create examples/typescript/informdata-webhook-demo/ with .env.example and a webhook replay script; ensure it compiles and validates a sample event locally.
- Link the guide from docs/sales-collateral/README.md under “Technical references”; update docs site navigation and run docs build/deploy if applicable.
- Run CI link checks and Markdown linting.

Notes:
- During expansion, log and reconcile any SDK/API discrepancies; standardize on canonical field names and headers.
</info added on 2025-11-03T02:19:22.320Z>

